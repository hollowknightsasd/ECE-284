{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "radical-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "VGG_quant(\n",
      "  (features): Sequential(\n",
      "    (0): QuantConv2d(\n",
      "      3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): QuantConv2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): QuantConv2d(\n",
      "      64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): QuantConv2d(\n",
      "      128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): QuantConv2d(\n",
      "      128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): QuantConv2d(\n",
      "      256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (25): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): QuantConv2d(\n",
      "      8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (28): Sequential()\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): QuantConv2d(\n",
      "      8, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU(inplace=True)\n",
      "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (34): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (36): ReLU(inplace=True)\n",
      "    (37): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (39): ReLU(inplace=True)\n",
      "    (40): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU(inplace=True)\n",
      "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (44): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "     \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *\n",
    "\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "    \n",
    "    \n",
    "    \n",
    "batch_size = 128\n",
    "model_name = \"VGG16_quant4bit\"\n",
    "model = VGG16_quant()\n",
    "model.features[24] = QuantConv2d(256, 8, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "model.features[25] = nn.BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "model.features[27] = QuantConv2d(8, 8, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "model.features[28] = nn.Sequential()\n",
    "model.features[30] = QuantConv2d(8, 512, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#model.features[24] = QuantConv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#model.features[26] = QuantConv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#model.features[28] = QuantConv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "print(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [60, 80]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1        \n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "junior-reminder",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tTime 0.409 (0.409)\tData 0.242 (0.242)\tLoss 2.3846 (2.3846)\tPrec 10.156% (10.156%)\n",
      "Epoch: [0][100/391]\tTime 0.055 (0.058)\tData 0.002 (0.004)\tLoss 2.3539 (3.0093)\tPrec 14.062% (11.889%)\n",
      "Epoch: [0][200/391]\tTime 0.054 (0.056)\tData 0.001 (0.003)\tLoss 2.1668 (2.5918)\tPrec 17.969% (14.385%)\n",
      "Epoch: [0][300/391]\tTime 0.054 (0.056)\tData 0.001 (0.002)\tLoss 2.0692 (2.4084)\tPrec 22.656% (16.323%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.241 (0.241)\tLoss 2.0212 (2.0212)\tPrec 24.219% (24.219%)\n",
      " * Prec 22.950% \n",
      "best acc: 22.950000\n",
      "Epoch: [1][0/391]\tTime 0.264 (0.264)\tData 0.212 (0.212)\tLoss 1.7874 (1.7874)\tPrec 28.906% (28.906%)\n",
      "Epoch: [1][100/391]\tTime 0.058 (0.059)\tData 0.002 (0.004)\tLoss 1.9209 (1.9188)\tPrec 21.094% (23.987%)\n",
      "Epoch: [1][200/391]\tTime 0.057 (0.057)\tData 0.002 (0.003)\tLoss 1.9143 (1.9014)\tPrec 26.562% (24.510%)\n",
      "Epoch: [1][300/391]\tTime 0.059 (0.057)\tData 0.002 (0.002)\tLoss 1.8668 (1.8936)\tPrec 27.344% (24.912%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.199 (0.199)\tLoss 1.8372 (1.8372)\tPrec 28.906% (28.906%)\n",
      " * Prec 28.320% \n",
      "best acc: 28.320000\n",
      "Epoch: [2][0/391]\tTime 0.279 (0.279)\tData 0.217 (0.217)\tLoss 1.8545 (1.8545)\tPrec 31.250% (31.250%)\n",
      "Epoch: [2][100/391]\tTime 0.055 (0.057)\tData 0.002 (0.004)\tLoss 1.7258 (1.8012)\tPrec 34.375% (30.701%)\n",
      "Epoch: [2][200/391]\tTime 0.057 (0.057)\tData 0.002 (0.003)\tLoss 1.7187 (1.7903)\tPrec 32.812% (30.597%)\n",
      "Epoch: [2][300/391]\tTime 0.055 (0.057)\tData 0.002 (0.002)\tLoss 1.6987 (1.7731)\tPrec 32.031% (31.554%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.244 (0.244)\tLoss 1.6547 (1.6547)\tPrec 39.062% (39.062%)\n",
      " * Prec 33.040% \n",
      "best acc: 33.040000\n",
      "Epoch: [3][0/391]\tTime 0.284 (0.284)\tData 0.230 (0.230)\tLoss 1.7388 (1.7388)\tPrec 36.719% (36.719%)\n",
      "Epoch: [3][100/391]\tTime 0.054 (0.059)\tData 0.001 (0.004)\tLoss 1.6285 (1.6534)\tPrec 36.719% (35.860%)\n",
      "Epoch: [3][200/391]\tTime 0.053 (0.057)\tData 0.002 (0.003)\tLoss 1.5552 (1.6370)\tPrec 42.188% (36.641%)\n",
      "Epoch: [3][300/391]\tTime 0.056 (0.056)\tData 0.002 (0.002)\tLoss 1.5782 (1.6174)\tPrec 42.188% (37.513%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.208 (0.208)\tLoss 1.7196 (1.7196)\tPrec 35.156% (35.156%)\n",
      " * Prec 36.230% \n",
      "best acc: 36.230000\n",
      "Epoch: [4][0/391]\tTime 0.251 (0.251)\tData 0.203 (0.203)\tLoss 1.5380 (1.5380)\tPrec 42.188% (42.188%)\n",
      "Epoch: [4][100/391]\tTime 0.055 (0.057)\tData 0.002 (0.004)\tLoss 1.4363 (1.5182)\tPrec 50.000% (42.404%)\n",
      "Epoch: [4][200/391]\tTime 0.055 (0.056)\tData 0.002 (0.003)\tLoss 1.5221 (1.4972)\tPrec 46.094% (43.412%)\n",
      "Epoch: [4][300/391]\tTime 0.055 (0.056)\tData 0.002 (0.002)\tLoss 1.5234 (1.4787)\tPrec 42.188% (44.417%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.233 (0.233)\tLoss 1.3222 (1.3222)\tPrec 45.312% (45.312%)\n",
      " * Prec 46.650% \n",
      "best acc: 46.650000\n",
      "Epoch: [5][0/391]\tTime 0.265 (0.265)\tData 0.221 (0.221)\tLoss 1.4243 (1.4243)\tPrec 46.094% (46.094%)\n",
      "Epoch: [5][100/391]\tTime 0.053 (0.057)\tData 0.002 (0.004)\tLoss 1.2855 (1.3604)\tPrec 57.031% (49.961%)\n",
      "Epoch: [5][200/391]\tTime 0.055 (0.056)\tData 0.002 (0.003)\tLoss 1.2400 (1.3507)\tPrec 56.250% (50.536%)\n",
      "Epoch: [5][300/391]\tTime 0.054 (0.056)\tData 0.002 (0.002)\tLoss 1.1976 (1.3270)\tPrec 57.812% (51.204%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.202 (0.202)\tLoss 1.1099 (1.1099)\tPrec 62.500% (62.500%)\n",
      " * Prec 54.070% \n",
      "best acc: 54.070000\n",
      "Epoch: [6][0/391]\tTime 0.265 (0.265)\tData 0.226 (0.226)\tLoss 1.1275 (1.1275)\tPrec 63.281% (63.281%)\n",
      "Epoch: [6][100/391]\tTime 0.053 (0.057)\tData 0.002 (0.004)\tLoss 1.1724 (1.2075)\tPrec 57.812% (56.915%)\n",
      "Epoch: [6][200/391]\tTime 0.056 (0.056)\tData 0.001 (0.003)\tLoss 1.4105 (1.2049)\tPrec 52.344% (56.713%)\n",
      "Epoch: [6][300/391]\tTime 0.053 (0.056)\tData 0.001 (0.002)\tLoss 1.0673 (1.1899)\tPrec 61.719% (57.247%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.226 (0.226)\tLoss 1.0966 (1.0966)\tPrec 63.281% (63.281%)\n",
      " * Prec 56.990% \n",
      "best acc: 56.990000\n",
      "Epoch: [7][0/391]\tTime 0.237 (0.237)\tData 0.200 (0.200)\tLoss 1.3164 (1.3164)\tPrec 56.250% (56.250%)\n",
      "Epoch: [7][100/391]\tTime 0.056 (0.057)\tData 0.001 (0.004)\tLoss 1.1325 (1.1179)\tPrec 54.688% (60.365%)\n",
      "Epoch: [7][200/391]\tTime 0.051 (0.055)\tData 0.001 (0.002)\tLoss 0.9232 (1.1096)\tPrec 67.188% (60.187%)\n",
      "Epoch: [7][300/391]\tTime 0.054 (0.055)\tData 0.002 (0.002)\tLoss 1.0576 (1.0969)\tPrec 57.031% (60.675%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.199 (0.199)\tLoss 1.0309 (1.0309)\tPrec 66.406% (66.406%)\n",
      " * Prec 60.820% \n",
      "best acc: 60.820000\n",
      "Epoch: [8][0/391]\tTime 0.267 (0.267)\tData 0.218 (0.218)\tLoss 0.9442 (0.9442)\tPrec 67.969% (67.969%)\n",
      "Epoch: [8][100/391]\tTime 0.054 (0.057)\tData 0.002 (0.004)\tLoss 0.9704 (1.0039)\tPrec 65.625% (63.993%)\n",
      "Epoch: [8][200/391]\tTime 0.053 (0.056)\tData 0.001 (0.003)\tLoss 1.0403 (0.9964)\tPrec 59.375% (64.486%)\n",
      "Epoch: [8][300/391]\tTime 0.053 (0.055)\tData 0.002 (0.002)\tLoss 1.0532 (0.9900)\tPrec 61.719% (64.626%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.208 (0.208)\tLoss 0.9344 (0.9344)\tPrec 67.188% (67.188%)\n",
      " * Prec 65.700% \n",
      "best acc: 65.700000\n",
      "Epoch: [9][0/391]\tTime 0.234 (0.234)\tData 0.194 (0.194)\tLoss 0.8420 (0.8420)\tPrec 69.531% (69.531%)\n",
      "Epoch: [9][100/391]\tTime 0.057 (0.057)\tData 0.002 (0.004)\tLoss 0.8937 (0.9246)\tPrec 70.312% (66.901%)\n",
      "Epoch: [9][200/391]\tTime 0.059 (0.056)\tData 0.002 (0.003)\tLoss 1.1065 (0.9300)\tPrec 64.844% (67.005%)\n",
      "Epoch: [9][300/391]\tTime 0.055 (0.056)\tData 0.002 (0.002)\tLoss 0.8946 (0.9231)\tPrec 66.406% (67.206%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.222 (0.222)\tLoss 0.8512 (0.8512)\tPrec 64.844% (64.844%)\n",
      " * Prec 66.350% \n",
      "best acc: 66.350000\n",
      "Epoch: [10][0/391]\tTime 0.305 (0.305)\tData 0.256 (0.256)\tLoss 0.8171 (0.8171)\tPrec 71.875% (71.875%)\n",
      "Epoch: [10][100/391]\tTime 0.054 (0.057)\tData 0.001 (0.004)\tLoss 0.8730 (0.8658)\tPrec 67.969% (69.539%)\n",
      "Epoch: [10][200/391]\tTime 0.052 (0.057)\tData 0.001 (0.003)\tLoss 1.0068 (0.8641)\tPrec 67.188% (69.648%)\n",
      "Epoch: [10][300/391]\tTime 0.053 (0.056)\tData 0.002 (0.003)\tLoss 0.8638 (0.8611)\tPrec 67.188% (69.708%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.222 (0.222)\tLoss 0.9082 (0.9082)\tPrec 69.531% (69.531%)\n",
      " * Prec 66.760% \n",
      "best acc: 66.760000\n",
      "Epoch: [11][0/391]\tTime 0.277 (0.277)\tData 0.234 (0.234)\tLoss 0.8056 (0.8056)\tPrec 74.219% (74.219%)\n",
      "Epoch: [11][100/391]\tTime 0.057 (0.058)\tData 0.001 (0.004)\tLoss 0.7777 (0.8210)\tPrec 71.094% (71.914%)\n",
      "Epoch: [11][200/391]\tTime 0.056 (0.057)\tData 0.002 (0.003)\tLoss 0.8569 (0.8083)\tPrec 71.094% (72.038%)\n",
      "Epoch: [11][300/391]\tTime 0.057 (0.056)\tData 0.002 (0.002)\tLoss 0.9132 (0.8071)\tPrec 66.406% (71.971%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.223 (0.223)\tLoss 0.8094 (0.8094)\tPrec 69.531% (69.531%)\n",
      " * Prec 69.320% \n",
      "best acc: 69.320000\n",
      "Epoch: [12][0/391]\tTime 0.235 (0.235)\tData 0.193 (0.193)\tLoss 0.8251 (0.8251)\tPrec 70.312% (70.312%)\n",
      "Epoch: [12][100/391]\tTime 0.057 (0.059)\tData 0.001 (0.004)\tLoss 0.8530 (0.7700)\tPrec 75.781% (73.414%)\n",
      "Epoch: [12][200/391]\tTime 0.056 (0.057)\tData 0.001 (0.003)\tLoss 0.7038 (0.7606)\tPrec 75.781% (73.453%)\n",
      "Epoch: [12][300/391]\tTime 0.056 (0.056)\tData 0.002 (0.002)\tLoss 0.6465 (0.7634)\tPrec 75.781% (73.360%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.214 (0.214)\tLoss 0.7446 (0.7446)\tPrec 75.000% (75.000%)\n",
      " * Prec 72.120% \n",
      "best acc: 72.120000\n",
      "Epoch: [13][0/391]\tTime 0.285 (0.285)\tData 0.246 (0.246)\tLoss 0.7280 (0.7280)\tPrec 74.219% (74.219%)\n",
      "Epoch: [13][100/391]\tTime 0.055 (0.058)\tData 0.001 (0.004)\tLoss 0.7950 (0.7150)\tPrec 68.750% (75.541%)\n",
      "Epoch: [13][200/391]\tTime 0.055 (0.057)\tData 0.001 (0.003)\tLoss 0.5378 (0.7240)\tPrec 79.688% (75.117%)\n",
      "Epoch: [13][300/391]\tTime 0.055 (0.056)\tData 0.001 (0.002)\tLoss 0.6436 (0.7301)\tPrec 78.125% (75.005%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.199 (0.199)\tLoss 0.7461 (0.7461)\tPrec 74.219% (74.219%)\n",
      " * Prec 71.120% \n",
      "best acc: 72.120000\n",
      "Epoch: [14][0/391]\tTime 0.265 (0.265)\tData 0.218 (0.218)\tLoss 0.8804 (0.8804)\tPrec 71.875% (71.875%)\n",
      "Epoch: [14][100/391]\tTime 0.050 (0.057)\tData 0.002 (0.004)\tLoss 0.6579 (0.7025)\tPrec 75.000% (76.153%)\n",
      "Epoch: [14][200/391]\tTime 0.055 (0.056)\tData 0.002 (0.003)\tLoss 0.6763 (0.6977)\tPrec 75.781% (76.275%)\n",
      "Epoch: [14][300/391]\tTime 0.054 (0.056)\tData 0.001 (0.002)\tLoss 0.6387 (0.6911)\tPrec 76.562% (76.498%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.207 (0.207)\tLoss 0.7366 (0.7366)\tPrec 72.656% (72.656%)\n",
      " * Prec 75.280% \n",
      "best acc: 75.280000\n",
      "Epoch: [15][0/391]\tTime 0.303 (0.303)\tData 0.254 (0.254)\tLoss 0.5262 (0.5262)\tPrec 80.469% (80.469%)\n",
      "Epoch: [15][100/391]\tTime 0.053 (0.058)\tData 0.002 (0.004)\tLoss 0.5531 (0.6491)\tPrec 82.812% (77.684%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][200/391]\tTime 0.055 (0.056)\tData 0.001 (0.003)\tLoss 0.6446 (0.6516)\tPrec 82.812% (77.713%)\n",
      "Epoch: [15][300/391]\tTime 0.057 (0.056)\tData 0.001 (0.002)\tLoss 0.6164 (0.6526)\tPrec 76.562% (77.634%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.213 (0.213)\tLoss 0.6141 (0.6141)\tPrec 79.688% (79.688%)\n",
      " * Prec 76.260% \n",
      "best acc: 76.260000\n",
      "Epoch: [16][0/391]\tTime 0.288 (0.288)\tData 0.237 (0.237)\tLoss 0.5688 (0.5688)\tPrec 82.031% (82.031%)\n",
      "Epoch: [16][100/391]\tTime 0.057 (0.057)\tData 0.002 (0.004)\tLoss 0.6380 (0.6152)\tPrec 79.688% (79.138%)\n",
      "Epoch: [16][200/391]\tTime 0.055 (0.056)\tData 0.001 (0.003)\tLoss 0.7263 (0.6175)\tPrec 73.438% (78.910%)\n",
      "Epoch: [16][300/391]\tTime 0.055 (0.056)\tData 0.002 (0.002)\tLoss 0.7878 (0.6209)\tPrec 75.000% (78.880%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.224 (0.224)\tLoss 0.6503 (0.6503)\tPrec 79.688% (79.688%)\n",
      " * Prec 76.210% \n",
      "best acc: 76.260000\n",
      "Epoch: [17][0/391]\tTime 0.294 (0.294)\tData 0.247 (0.247)\tLoss 0.5400 (0.5400)\tPrec 80.469% (80.469%)\n",
      "Epoch: [17][100/391]\tTime 0.050 (0.058)\tData 0.002 (0.004)\tLoss 0.4293 (0.5946)\tPrec 83.594% (79.834%)\n",
      "Epoch: [17][200/391]\tTime 0.059 (0.056)\tData 0.002 (0.003)\tLoss 0.6290 (0.5971)\tPrec 77.344% (79.629%)\n",
      "Epoch: [17][300/391]\tTime 0.055 (0.056)\tData 0.002 (0.003)\tLoss 0.6252 (0.5957)\tPrec 78.125% (79.825%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.247 (0.247)\tLoss 0.7067 (0.7067)\tPrec 76.562% (76.562%)\n",
      " * Prec 75.620% \n",
      "best acc: 76.260000\n",
      "Epoch: [18][0/391]\tTime 0.283 (0.283)\tData 0.239 (0.239)\tLoss 0.4576 (0.4576)\tPrec 82.812% (82.812%)\n",
      "Epoch: [18][100/391]\tTime 0.053 (0.058)\tData 0.003 (0.004)\tLoss 0.5142 (0.5806)\tPrec 79.688% (80.159%)\n",
      "Epoch: [18][200/391]\tTime 0.054 (0.057)\tData 0.002 (0.003)\tLoss 0.5655 (0.5779)\tPrec 78.906% (80.461%)\n",
      "Epoch: [18][300/391]\tTime 0.055 (0.056)\tData 0.001 (0.002)\tLoss 0.6351 (0.5763)\tPrec 77.344% (80.477%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.199 (0.199)\tLoss 0.6843 (0.6843)\tPrec 81.250% (81.250%)\n",
      " * Prec 75.640% \n",
      "best acc: 76.260000\n",
      "Epoch: [19][0/391]\tTime 0.224 (0.224)\tData 0.176 (0.176)\tLoss 0.4557 (0.4557)\tPrec 83.594% (83.594%)\n",
      "Epoch: [19][100/391]\tTime 0.049 (0.057)\tData 0.001 (0.003)\tLoss 0.6259 (0.5621)\tPrec 74.219% (81.126%)\n",
      "Epoch: [19][200/391]\tTime 0.052 (0.056)\tData 0.002 (0.002)\tLoss 0.5215 (0.5586)\tPrec 80.469% (81.126%)\n",
      "Epoch: [19][300/391]\tTime 0.054 (0.056)\tData 0.001 (0.002)\tLoss 0.4182 (0.5598)\tPrec 85.938% (81.102%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.242 (0.242)\tLoss 0.7021 (0.7021)\tPrec 78.125% (78.125%)\n",
      " * Prec 76.870% \n",
      "best acc: 76.870000\n",
      "Epoch: [20][0/391]\tTime 0.252 (0.252)\tData 0.205 (0.205)\tLoss 0.6467 (0.6467)\tPrec 76.562% (76.562%)\n",
      "Epoch: [20][100/391]\tTime 0.055 (0.057)\tData 0.002 (0.004)\tLoss 0.5915 (0.5278)\tPrec 82.812% (82.379%)\n",
      "Epoch: [20][200/391]\tTime 0.056 (0.056)\tData 0.002 (0.003)\tLoss 0.6274 (0.5279)\tPrec 76.562% (82.183%)\n",
      "Epoch: [20][300/391]\tTime 0.059 (0.056)\tData 0.002 (0.002)\tLoss 0.5563 (0.5255)\tPrec 80.469% (82.143%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.227 (0.227)\tLoss 0.5825 (0.5825)\tPrec 78.906% (78.906%)\n",
      " * Prec 76.480% \n",
      "best acc: 76.870000\n",
      "Epoch: [21][0/391]\tTime 0.275 (0.275)\tData 0.226 (0.226)\tLoss 0.5466 (0.5466)\tPrec 80.469% (80.469%)\n",
      "Epoch: [21][100/391]\tTime 0.054 (0.057)\tData 0.001 (0.004)\tLoss 0.5818 (0.5252)\tPrec 77.344% (82.410%)\n",
      "Epoch: [21][200/391]\tTime 0.054 (0.056)\tData 0.001 (0.003)\tLoss 0.4497 (0.5220)\tPrec 84.375% (82.525%)\n",
      "Epoch: [21][300/391]\tTime 0.054 (0.056)\tData 0.001 (0.002)\tLoss 0.6210 (0.5221)\tPrec 85.938% (82.413%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.191 (0.191)\tLoss 0.5223 (0.5223)\tPrec 78.906% (78.906%)\n",
      " * Prec 79.530% \n",
      "best acc: 79.530000\n",
      "Epoch: [22][0/391]\tTime 0.279 (0.279)\tData 0.230 (0.230)\tLoss 0.3922 (0.3922)\tPrec 85.938% (85.938%)\n",
      "Epoch: [22][100/391]\tTime 0.055 (0.058)\tData 0.002 (0.004)\tLoss 0.5122 (0.4855)\tPrec 79.688% (83.362%)\n",
      "Epoch: [22][200/391]\tTime 0.057 (0.057)\tData 0.003 (0.003)\tLoss 0.5561 (0.4878)\tPrec 82.031% (83.442%)\n",
      "Epoch: [22][300/391]\tTime 0.053 (0.056)\tData 0.002 (0.003)\tLoss 0.4787 (0.4915)\tPrec 82.812% (83.467%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.215 (0.215)\tLoss 0.6169 (0.6169)\tPrec 78.906% (78.906%)\n",
      " * Prec 80.050% \n",
      "best acc: 80.050000\n",
      "Epoch: [23][0/391]\tTime 0.258 (0.258)\tData 0.208 (0.208)\tLoss 0.4081 (0.4081)\tPrec 87.500% (87.500%)\n",
      "Epoch: [23][100/391]\tTime 0.054 (0.057)\tData 0.001 (0.004)\tLoss 0.5371 (0.4670)\tPrec 82.812% (83.911%)\n",
      "Epoch: [23][200/391]\tTime 0.057 (0.057)\tData 0.002 (0.002)\tLoss 0.4969 (0.4737)\tPrec 81.250% (83.730%)\n",
      "Epoch: [23][300/391]\tTime 0.054 (0.056)\tData 0.002 (0.002)\tLoss 0.4277 (0.4766)\tPrec 83.594% (83.721%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.206 (0.206)\tLoss 0.5653 (0.5653)\tPrec 79.688% (79.688%)\n",
      " * Prec 78.530% \n",
      "best acc: 80.050000\n",
      "Epoch: [24][0/391]\tTime 0.261 (0.261)\tData 0.215 (0.215)\tLoss 0.4436 (0.4436)\tPrec 84.375% (84.375%)\n",
      "Epoch: [24][100/391]\tTime 0.055 (0.058)\tData 0.002 (0.004)\tLoss 0.3637 (0.4698)\tPrec 85.938% (84.019%)\n",
      "Epoch: [24][200/391]\tTime 0.056 (0.057)\tData 0.001 (0.003)\tLoss 0.5716 (0.4693)\tPrec 82.031% (84.173%)\n",
      "Epoch: [24][300/391]\tTime 0.054 (0.056)\tData 0.002 (0.002)\tLoss 0.4931 (0.4676)\tPrec 82.031% (84.253%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.227 (0.227)\tLoss 0.7080 (0.7080)\tPrec 75.000% (75.000%)\n",
      " * Prec 78.370% \n",
      "best acc: 80.050000\n",
      "Epoch: [25][0/391]\tTime 0.237 (0.237)\tData 0.190 (0.190)\tLoss 0.4125 (0.4125)\tPrec 82.812% (82.812%)\n",
      "Epoch: [25][100/391]\tTime 0.054 (0.057)\tData 0.002 (0.004)\tLoss 0.4694 (0.4315)\tPrec 84.375% (85.179%)\n",
      "Epoch: [25][200/391]\tTime 0.059 (0.056)\tData 0.001 (0.003)\tLoss 0.5074 (0.4446)\tPrec 82.031% (84.612%)\n",
      "Epoch: [25][300/391]\tTime 0.055 (0.056)\tData 0.002 (0.002)\tLoss 0.5177 (0.4438)\tPrec 82.031% (84.686%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.225 (0.225)\tLoss 0.5876 (0.5876)\tPrec 83.594% (83.594%)\n",
      " * Prec 81.310% \n",
      "best acc: 81.310000\n",
      "Epoch: [26][0/391]\tTime 0.288 (0.288)\tData 0.239 (0.239)\tLoss 0.3989 (0.3989)\tPrec 82.812% (82.812%)\n",
      "Epoch: [26][100/391]\tTime 0.055 (0.057)\tData 0.001 (0.004)\tLoss 0.4005 (0.4191)\tPrec 85.156% (85.729%)\n",
      "Epoch: [26][200/391]\tTime 0.057 (0.056)\tData 0.002 (0.003)\tLoss 0.4420 (0.4265)\tPrec 83.594% (85.510%)\n",
      "Epoch: [26][300/391]\tTime 0.056 (0.056)\tData 0.002 (0.002)\tLoss 0.5334 (0.4308)\tPrec 80.469% (85.379%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.221 (0.221)\tLoss 0.5454 (0.5454)\tPrec 80.469% (80.469%)\n",
      " * Prec 80.820% \n",
      "best acc: 81.310000\n",
      "Epoch: [27][0/391]\tTime 0.306 (0.306)\tData 0.258 (0.258)\tLoss 0.2939 (0.2939)\tPrec 92.188% (92.188%)\n",
      "Epoch: [27][100/391]\tTime 0.059 (0.058)\tData 0.002 (0.004)\tLoss 0.5050 (0.3977)\tPrec 82.031% (86.603%)\n",
      "Epoch: [27][200/391]\tTime 0.053 (0.056)\tData 0.001 (0.003)\tLoss 0.5080 (0.4045)\tPrec 82.031% (86.392%)\n",
      "Epoch: [27][300/391]\tTime 0.053 (0.056)\tData 0.001 (0.002)\tLoss 0.5381 (0.4101)\tPrec 85.938% (86.119%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.221 (0.221)\tLoss 0.5349 (0.5349)\tPrec 82.812% (82.812%)\n",
      " * Prec 81.470% \n",
      "best acc: 81.470000\n",
      "Epoch: [28][0/391]\tTime 0.240 (0.240)\tData 0.198 (0.198)\tLoss 0.3404 (0.3404)\tPrec 87.500% (87.500%)\n",
      "Epoch: [28][100/391]\tTime 0.057 (0.058)\tData 0.002 (0.004)\tLoss 0.4359 (0.3837)\tPrec 84.375% (87.152%)\n",
      "Epoch: [28][200/391]\tTime 0.055 (0.057)\tData 0.001 (0.003)\tLoss 0.2885 (0.3963)\tPrec 87.500% (86.524%)\n",
      "Epoch: [28][300/391]\tTime 0.056 (0.056)\tData 0.002 (0.002)\tLoss 0.4454 (0.4002)\tPrec 81.250% (86.342%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.218 (0.218)\tLoss 0.4651 (0.4651)\tPrec 85.938% (85.938%)\n",
      " * Prec 80.890% \n",
      "best acc: 81.470000\n",
      "Epoch: [29][0/391]\tTime 0.281 (0.281)\tData 0.235 (0.235)\tLoss 0.3338 (0.3338)\tPrec 89.062% (89.062%)\n",
      "Epoch: [29][100/391]\tTime 0.054 (0.059)\tData 0.004 (0.004)\tLoss 0.4338 (0.3852)\tPrec 85.156% (86.989%)\n",
      "Epoch: [29][200/391]\tTime 0.058 (0.058)\tData 0.001 (0.003)\tLoss 0.3549 (0.3852)\tPrec 88.281% (86.863%)\n",
      "Epoch: [29][300/391]\tTime 0.094 (0.060)\tData 0.002 (0.002)\tLoss 0.4800 (0.3849)\tPrec 82.812% (86.942%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.224 (0.224)\tLoss 0.5590 (0.5590)\tPrec 83.594% (83.594%)\n",
      " * Prec 81.140% \n",
      "best acc: 81.470000\n",
      "Epoch: [30][0/391]\tTime 0.267 (0.267)\tData 0.208 (0.208)\tLoss 0.3628 (0.3628)\tPrec 86.719% (86.719%)\n",
      "Epoch: [30][100/391]\tTime 0.085 (0.086)\tData 0.002 (0.004)\tLoss 0.3686 (0.3610)\tPrec 87.500% (87.833%)\n",
      "Epoch: [30][200/391]\tTime 0.083 (0.085)\tData 0.001 (0.003)\tLoss 0.4295 (0.3745)\tPrec 89.062% (87.181%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][300/391]\tTime 0.085 (0.084)\tData 0.002 (0.002)\tLoss 0.3262 (0.3766)\tPrec 86.719% (87.256%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.231 (0.231)\tLoss 0.4799 (0.4799)\tPrec 82.031% (82.031%)\n",
      " * Prec 80.690% \n",
      "best acc: 81.470000\n",
      "Epoch: [31][0/391]\tTime 0.298 (0.298)\tData 0.245 (0.245)\tLoss 0.2130 (0.2130)\tPrec 93.750% (93.750%)\n",
      "Epoch: [31][100/391]\tTime 0.085 (0.087)\tData 0.002 (0.004)\tLoss 0.4575 (0.3508)\tPrec 82.812% (88.034%)\n",
      "Epoch: [31][200/391]\tTime 0.088 (0.085)\tData 0.001 (0.003)\tLoss 0.2649 (0.3574)\tPrec 92.188% (87.885%)\n",
      "Epoch: [31][300/391]\tTime 0.086 (0.085)\tData 0.002 (0.002)\tLoss 0.2878 (0.3624)\tPrec 89.062% (87.573%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.210 (0.210)\tLoss 0.4317 (0.4317)\tPrec 82.812% (82.812%)\n",
      " * Prec 82.780% \n",
      "best acc: 82.780000\n",
      "Epoch: [32][0/391]\tTime 0.303 (0.303)\tData 0.237 (0.237)\tLoss 0.3235 (0.3235)\tPrec 92.188% (92.188%)\n",
      "Epoch: [32][100/391]\tTime 0.088 (0.087)\tData 0.002 (0.004)\tLoss 0.2945 (0.3494)\tPrec 89.062% (88.475%)\n",
      "Epoch: [32][200/391]\tTime 0.083 (0.085)\tData 0.002 (0.003)\tLoss 0.3845 (0.3524)\tPrec 89.062% (88.172%)\n",
      "Epoch: [32][300/391]\tTime 0.084 (0.085)\tData 0.001 (0.003)\tLoss 0.2849 (0.3535)\tPrec 92.188% (88.097%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.196 (0.196)\tLoss 0.5222 (0.5222)\tPrec 82.031% (82.031%)\n",
      " * Prec 83.850% \n",
      "best acc: 83.850000\n",
      "Epoch: [33][0/391]\tTime 0.244 (0.244)\tData 0.184 (0.184)\tLoss 0.4320 (0.4320)\tPrec 85.156% (85.156%)\n",
      "Epoch: [33][100/391]\tTime 0.081 (0.086)\tData 0.002 (0.003)\tLoss 0.2519 (0.3312)\tPrec 92.969% (88.745%)\n",
      "Epoch: [33][200/391]\tTime 0.084 (0.085)\tData 0.002 (0.003)\tLoss 0.4076 (0.3410)\tPrec 87.500% (88.351%)\n",
      "Epoch: [33][300/391]\tTime 0.083 (0.085)\tData 0.002 (0.002)\tLoss 0.3461 (0.3420)\tPrec 86.719% (88.268%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.216 (0.216)\tLoss 0.4439 (0.4439)\tPrec 78.906% (78.906%)\n",
      " * Prec 83.750% \n",
      "best acc: 83.850000\n",
      "Epoch: [34][0/391]\tTime 0.308 (0.308)\tData 0.256 (0.256)\tLoss 0.2288 (0.2288)\tPrec 92.969% (92.969%)\n",
      "Epoch: [34][100/391]\tTime 0.085 (0.084)\tData 0.002 (0.004)\tLoss 0.4197 (0.3319)\tPrec 86.719% (88.420%)\n",
      "Epoch: [34][200/391]\tTime 0.083 (0.084)\tData 0.001 (0.003)\tLoss 0.3448 (0.3298)\tPrec 88.281% (88.647%)\n",
      "Epoch: [34][300/391]\tTime 0.086 (0.084)\tData 0.001 (0.002)\tLoss 0.3279 (0.3310)\tPrec 85.938% (88.655%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.213 (0.213)\tLoss 0.5201 (0.5201)\tPrec 83.594% (83.594%)\n",
      " * Prec 84.030% \n",
      "best acc: 84.030000\n",
      "Epoch: [35][0/391]\tTime 0.275 (0.275)\tData 0.206 (0.206)\tLoss 0.2561 (0.2561)\tPrec 92.969% (92.969%)\n",
      "Epoch: [35][100/391]\tTime 0.082 (0.085)\tData 0.002 (0.004)\tLoss 0.3374 (0.3050)\tPrec 89.844% (89.898%)\n",
      "Epoch: [35][200/391]\tTime 0.087 (0.085)\tData 0.002 (0.003)\tLoss 0.3992 (0.3147)\tPrec 88.281% (89.397%)\n",
      "Epoch: [35][300/391]\tTime 0.096 (0.085)\tData 0.002 (0.002)\tLoss 0.2651 (0.3174)\tPrec 91.406% (89.343%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.204 (0.204)\tLoss 0.5272 (0.5272)\tPrec 82.812% (82.812%)\n",
      " * Prec 82.320% \n",
      "best acc: 84.030000\n",
      "Epoch: [36][0/391]\tTime 0.318 (0.318)\tData 0.263 (0.263)\tLoss 0.2699 (0.2699)\tPrec 89.844% (89.844%)\n",
      "Epoch: [36][100/391]\tTime 0.086 (0.086)\tData 0.001 (0.004)\tLoss 0.2328 (0.3033)\tPrec 92.188% (89.782%)\n",
      "Epoch: [36][200/391]\tTime 0.087 (0.085)\tData 0.002 (0.003)\tLoss 0.2970 (0.3127)\tPrec 89.844% (89.533%)\n",
      "Epoch: [36][300/391]\tTime 0.084 (0.085)\tData 0.002 (0.003)\tLoss 0.3068 (0.3175)\tPrec 90.625% (89.325%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.214 (0.214)\tLoss 0.4056 (0.4056)\tPrec 86.719% (86.719%)\n",
      " * Prec 84.740% \n",
      "best acc: 84.740000\n",
      "Epoch: [37][0/391]\tTime 0.488 (0.488)\tData 0.437 (0.437)\tLoss 0.2396 (0.2396)\tPrec 91.406% (91.406%)\n",
      "Epoch: [37][100/391]\tTime 0.080 (0.087)\tData 0.002 (0.006)\tLoss 0.3977 (0.3001)\tPrec 88.281% (90.029%)\n",
      "Epoch: [37][200/391]\tTime 0.082 (0.086)\tData 0.002 (0.004)\tLoss 0.2895 (0.3039)\tPrec 90.625% (89.735%)\n",
      "Epoch: [37][300/391]\tTime 0.088 (0.086)\tData 0.002 (0.003)\tLoss 0.1774 (0.3042)\tPrec 90.625% (89.634%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.197 (0.197)\tLoss 0.5233 (0.5233)\tPrec 83.594% (83.594%)\n",
      " * Prec 83.630% \n",
      "best acc: 84.740000\n",
      "Epoch: [38][0/391]\tTime 0.266 (0.266)\tData 0.211 (0.211)\tLoss 0.2214 (0.2214)\tPrec 91.406% (91.406%)\n",
      "Epoch: [38][100/391]\tTime 0.086 (0.086)\tData 0.002 (0.004)\tLoss 0.3786 (0.2863)\tPrec 86.719% (90.447%)\n",
      "Epoch: [38][200/391]\tTime 0.094 (0.087)\tData 0.002 (0.003)\tLoss 0.2646 (0.2891)\tPrec 92.188% (90.213%)\n",
      "Epoch: [38][300/391]\tTime 0.085 (0.087)\tData 0.002 (0.002)\tLoss 0.2502 (0.2944)\tPrec 89.844% (89.961%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.198 (0.198)\tLoss 0.4058 (0.4058)\tPrec 85.156% (85.156%)\n",
      " * Prec 84.050% \n",
      "best acc: 84.740000\n",
      "Epoch: [39][0/391]\tTime 0.251 (0.251)\tData 0.203 (0.203)\tLoss 0.1995 (0.1995)\tPrec 92.188% (92.188%)\n",
      "Epoch: [39][100/391]\tTime 0.078 (0.088)\tData 0.002 (0.004)\tLoss 0.1489 (0.2822)\tPrec 94.531% (90.200%)\n",
      "Epoch: [39][200/391]\tTime 0.087 (0.086)\tData 0.002 (0.003)\tLoss 0.2324 (0.2865)\tPrec 90.625% (90.186%)\n",
      "Epoch: [39][300/391]\tTime 0.083 (0.086)\tData 0.002 (0.002)\tLoss 0.1789 (0.2875)\tPrec 93.750% (90.176%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.199 (0.199)\tLoss 0.4611 (0.4611)\tPrec 81.250% (81.250%)\n",
      " * Prec 83.750% \n",
      "best acc: 84.740000\n",
      "Epoch: [40][0/391]\tTime 0.272 (0.272)\tData 0.215 (0.215)\tLoss 0.2962 (0.2962)\tPrec 89.844% (89.844%)\n",
      "Epoch: [40][100/391]\tTime 0.082 (0.086)\tData 0.002 (0.004)\tLoss 0.3583 (0.2710)\tPrec 86.719% (90.934%)\n",
      "Epoch: [40][200/391]\tTime 0.087 (0.085)\tData 0.001 (0.003)\tLoss 0.2743 (0.2735)\tPrec 87.500% (90.831%)\n",
      "Epoch: [40][300/391]\tTime 0.093 (0.085)\tData 0.001 (0.002)\tLoss 0.3527 (0.2804)\tPrec 87.500% (90.472%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.204 (0.204)\tLoss 0.3466 (0.3466)\tPrec 91.406% (91.406%)\n",
      " * Prec 84.450% \n",
      "best acc: 84.740000\n",
      "Epoch: [41][0/391]\tTime 0.295 (0.295)\tData 0.233 (0.233)\tLoss 0.2086 (0.2086)\tPrec 94.531% (94.531%)\n",
      "Epoch: [41][100/391]\tTime 0.091 (0.087)\tData 0.002 (0.004)\tLoss 0.3681 (0.2658)\tPrec 85.938% (90.903%)\n",
      "Epoch: [41][200/391]\tTime 0.086 (0.086)\tData 0.001 (0.003)\tLoss 0.2414 (0.2665)\tPrec 91.406% (90.882%)\n",
      "Epoch: [41][300/391]\tTime 0.083 (0.085)\tData 0.002 (0.003)\tLoss 0.1594 (0.2738)\tPrec 93.750% (90.560%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.215 (0.215)\tLoss 0.4078 (0.4078)\tPrec 85.938% (85.938%)\n",
      " * Prec 82.930% \n",
      "best acc: 84.740000\n",
      "Epoch: [42][0/391]\tTime 0.278 (0.278)\tData 0.222 (0.222)\tLoss 0.2143 (0.2143)\tPrec 92.969% (92.969%)\n",
      "Epoch: [42][100/391]\tTime 0.083 (0.086)\tData 0.002 (0.004)\tLoss 0.2645 (0.2724)\tPrec 89.062% (90.586%)\n",
      "Epoch: [42][200/391]\tTime 0.083 (0.086)\tData 0.002 (0.003)\tLoss 0.3462 (0.2672)\tPrec 89.062% (90.804%)\n",
      "Epoch: [42][300/391]\tTime 0.086 (0.085)\tData 0.002 (0.002)\tLoss 0.2292 (0.2676)\tPrec 93.750% (90.908%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.223 (0.223)\tLoss 0.3845 (0.3845)\tPrec 88.281% (88.281%)\n",
      " * Prec 84.530% \n",
      "best acc: 84.740000\n",
      "Epoch: [43][0/391]\tTime 0.252 (0.252)\tData 0.199 (0.199)\tLoss 0.3314 (0.3314)\tPrec 88.281% (88.281%)\n",
      "Epoch: [43][100/391]\tTime 0.083 (0.088)\tData 0.002 (0.004)\tLoss 0.2841 (0.2399)\tPrec 89.844% (91.669%)\n",
      "Epoch: [43][200/391]\tTime 0.081 (0.086)\tData 0.001 (0.003)\tLoss 0.2310 (0.2428)\tPrec 89.062% (91.733%)\n",
      "Epoch: [43][300/391]\tTime 0.086 (0.086)\tData 0.002 (0.002)\tLoss 0.3501 (0.2495)\tPrec 87.500% (91.440%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.241 (0.241)\tLoss 0.2829 (0.2829)\tPrec 91.406% (91.406%)\n",
      " * Prec 84.980% \n",
      "best acc: 84.980000\n",
      "Epoch: [44][0/391]\tTime 0.300 (0.300)\tData 0.249 (0.249)\tLoss 0.3200 (0.3200)\tPrec 90.625% (90.625%)\n",
      "Epoch: [44][100/391]\tTime 0.086 (0.087)\tData 0.002 (0.004)\tLoss 0.2673 (0.2512)\tPrec 93.750% (91.290%)\n",
      "Epoch: [44][200/391]\tTime 0.089 (0.084)\tData 0.001 (0.003)\tLoss 0.3070 (0.2516)\tPrec 88.281% (91.332%)\n",
      "Epoch: [44][300/391]\tTime 0.099 (0.085)\tData 0.002 (0.003)\tLoss 0.4193 (0.2491)\tPrec 84.375% (91.323%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.207 (0.207)\tLoss 0.4378 (0.4378)\tPrec 88.281% (88.281%)\n",
      " * Prec 83.860% \n",
      "best acc: 84.980000\n",
      "Epoch: [45][0/391]\tTime 0.289 (0.289)\tData 0.234 (0.234)\tLoss 0.2215 (0.2215)\tPrec 92.188% (92.188%)\n",
      "Epoch: [45][100/391]\tTime 0.080 (0.087)\tData 0.002 (0.004)\tLoss 0.2329 (0.2284)\tPrec 90.625% (91.925%)\n",
      "Epoch: [45][200/391]\tTime 0.084 (0.086)\tData 0.001 (0.003)\tLoss 0.1549 (0.2356)\tPrec 94.531% (91.721%)\n",
      "Epoch: [45][300/391]\tTime 0.088 (0.086)\tData 0.002 (0.003)\tLoss 0.2952 (0.2391)\tPrec 92.188% (91.707%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation starts\n",
      "Test: [0/79]\tTime 0.187 (0.187)\tLoss 0.3883 (0.3883)\tPrec 87.500% (87.500%)\n",
      " * Prec 85.640% \n",
      "best acc: 85.640000\n",
      "Epoch: [46][0/391]\tTime 0.245 (0.245)\tData 0.184 (0.184)\tLoss 0.2375 (0.2375)\tPrec 92.969% (92.969%)\n",
      "Epoch: [46][100/391]\tTime 0.082 (0.089)\tData 0.002 (0.004)\tLoss 0.2633 (0.2214)\tPrec 89.062% (92.396%)\n",
      "Epoch: [46][200/391]\tTime 0.080 (0.087)\tData 0.002 (0.003)\tLoss 0.2156 (0.2318)\tPrec 92.188% (91.970%)\n",
      "Epoch: [46][300/391]\tTime 0.093 (0.087)\tData 0.002 (0.002)\tLoss 0.2348 (0.2356)\tPrec 92.188% (91.964%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.221 (0.221)\tLoss 0.4352 (0.4352)\tPrec 85.938% (85.938%)\n",
      " * Prec 84.150% \n",
      "best acc: 85.640000\n",
      "Epoch: [47][0/391]\tTime 0.255 (0.255)\tData 0.200 (0.200)\tLoss 0.1687 (0.1687)\tPrec 94.531% (94.531%)\n",
      "Epoch: [47][100/391]\tTime 0.086 (0.085)\tData 0.002 (0.004)\tLoss 0.2659 (0.2223)\tPrec 88.281% (92.304%)\n",
      "Epoch: [47][200/391]\tTime 0.083 (0.085)\tData 0.002 (0.003)\tLoss 0.1895 (0.2186)\tPrec 93.750% (92.355%)\n",
      "Epoch: [47][300/391]\tTime 0.084 (0.085)\tData 0.002 (0.002)\tLoss 0.2188 (0.2246)\tPrec 89.062% (92.154%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.230 (0.230)\tLoss 0.2883 (0.2883)\tPrec 88.281% (88.281%)\n",
      " * Prec 85.320% \n",
      "best acc: 85.640000\n",
      "Epoch: [48][0/391]\tTime 0.271 (0.271)\tData 0.213 (0.213)\tLoss 0.1736 (0.1736)\tPrec 92.969% (92.969%)\n",
      "Epoch: [48][100/391]\tTime 0.088 (0.087)\tData 0.002 (0.004)\tLoss 0.2648 (0.2020)\tPrec 89.844% (92.938%)\n",
      "Epoch: [48][200/391]\tTime 0.084 (0.086)\tData 0.002 (0.003)\tLoss 0.1610 (0.2131)\tPrec 94.531% (92.673%)\n",
      "Epoch: [48][300/391]\tTime 0.086 (0.086)\tData 0.002 (0.002)\tLoss 0.5157 (0.2177)\tPrec 85.156% (92.572%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.183 (0.183)\tLoss 0.2078 (0.2078)\tPrec 92.969% (92.969%)\n",
      " * Prec 85.960% \n",
      "best acc: 85.960000\n",
      "Epoch: [49][0/391]\tTime 0.291 (0.291)\tData 0.241 (0.241)\tLoss 0.2260 (0.2260)\tPrec 91.406% (91.406%)\n",
      "Epoch: [49][100/391]\tTime 0.080 (0.088)\tData 0.002 (0.004)\tLoss 0.2245 (0.2005)\tPrec 94.531% (93.085%)\n",
      "Epoch: [49][200/391]\tTime 0.091 (0.087)\tData 0.001 (0.003)\tLoss 0.1663 (0.2090)\tPrec 92.969% (92.712%)\n",
      "Epoch: [49][300/391]\tTime 0.085 (0.086)\tData 0.002 (0.003)\tLoss 0.1529 (0.2116)\tPrec 93.750% (92.709%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.217 (0.217)\tLoss 0.3850 (0.3850)\tPrec 85.938% (85.938%)\n",
      " * Prec 86.560% \n",
      "best acc: 86.560000\n",
      "Epoch: [50][0/391]\tTime 0.251 (0.251)\tData 0.202 (0.202)\tLoss 0.1274 (0.1274)\tPrec 95.312% (95.312%)\n",
      "Epoch: [50][100/391]\tTime 0.090 (0.087)\tData 0.001 (0.004)\tLoss 0.1647 (0.1916)\tPrec 95.312% (93.301%)\n",
      "Epoch: [50][200/391]\tTime 0.082 (0.086)\tData 0.002 (0.003)\tLoss 0.1429 (0.1998)\tPrec 93.750% (93.008%)\n",
      "Epoch: [50][300/391]\tTime 0.086 (0.085)\tData 0.001 (0.002)\tLoss 0.3169 (0.2044)\tPrec 91.406% (92.893%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.207 (0.207)\tLoss 0.3757 (0.3757)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.050% \n",
      "best acc: 86.560000\n",
      "Epoch: [51][0/391]\tTime 0.297 (0.297)\tData 0.242 (0.242)\tLoss 0.2066 (0.2066)\tPrec 92.188% (92.188%)\n",
      "Epoch: [51][100/391]\tTime 0.079 (0.088)\tData 0.001 (0.004)\tLoss 0.2532 (0.1897)\tPrec 92.188% (93.711%)\n",
      "Epoch: [51][200/391]\tTime 0.084 (0.086)\tData 0.002 (0.003)\tLoss 0.1934 (0.1969)\tPrec 92.969% (93.330%)\n",
      "Epoch: [51][300/391]\tTime 0.071 (0.085)\tData 0.002 (0.003)\tLoss 0.1437 (0.2025)\tPrec 95.312% (93.065%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.229 (0.229)\tLoss 0.3423 (0.3423)\tPrec 88.281% (88.281%)\n",
      " * Prec 85.680% \n",
      "best acc: 86.560000\n",
      "Epoch: [52][0/391]\tTime 0.283 (0.283)\tData 0.222 (0.222)\tLoss 0.1355 (0.1355)\tPrec 96.094% (96.094%)\n",
      "Epoch: [52][100/391]\tTime 0.080 (0.087)\tData 0.002 (0.004)\tLoss 0.2304 (0.1949)\tPrec 90.625% (93.232%)\n",
      "Epoch: [52][200/391]\tTime 0.084 (0.086)\tData 0.001 (0.003)\tLoss 0.2352 (0.1993)\tPrec 93.750% (93.132%)\n",
      "Epoch: [52][300/391]\tTime 0.085 (0.085)\tData 0.002 (0.003)\tLoss 0.3639 (0.1980)\tPrec 89.062% (93.192%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.245 (0.245)\tLoss 0.2655 (0.2655)\tPrec 86.719% (86.719%)\n",
      " * Prec 86.720% \n",
      "best acc: 86.720000\n",
      "Epoch: [53][0/391]\tTime 0.312 (0.312)\tData 0.248 (0.248)\tLoss 0.2155 (0.2155)\tPrec 89.844% (89.844%)\n",
      "Epoch: [53][100/391]\tTime 0.085 (0.087)\tData 0.002 (0.004)\tLoss 0.2424 (0.1942)\tPrec 91.406% (93.100%)\n",
      "Epoch: [53][200/391]\tTime 0.089 (0.086)\tData 0.002 (0.003)\tLoss 0.1621 (0.1936)\tPrec 91.406% (93.287%)\n",
      "Epoch: [53][300/391]\tTime 0.084 (0.085)\tData 0.001 (0.003)\tLoss 0.1808 (0.1926)\tPrec 95.312% (93.343%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.217 (0.217)\tLoss 0.3102 (0.3102)\tPrec 90.625% (90.625%)\n",
      " * Prec 86.400% \n",
      "best acc: 86.720000\n",
      "Epoch: [54][0/391]\tTime 0.256 (0.256)\tData 0.194 (0.194)\tLoss 0.1932 (0.1932)\tPrec 96.094% (96.094%)\n",
      "Epoch: [54][100/391]\tTime 0.085 (0.087)\tData 0.001 (0.004)\tLoss 0.2602 (0.1905)\tPrec 89.062% (93.502%)\n",
      "Epoch: [54][200/391]\tTime 0.061 (0.085)\tData 0.002 (0.003)\tLoss 0.1179 (0.1908)\tPrec 96.875% (93.435%)\n",
      "Epoch: [54][300/391]\tTime 0.076 (0.085)\tData 0.002 (0.002)\tLoss 0.1359 (0.1891)\tPrec 93.750% (93.498%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.240 (0.240)\tLoss 0.3646 (0.3646)\tPrec 87.500% (87.500%)\n",
      " * Prec 87.160% \n",
      "best acc: 87.160000\n",
      "Epoch: [55][0/391]\tTime 0.317 (0.317)\tData 0.258 (0.258)\tLoss 0.3616 (0.3616)\tPrec 87.500% (87.500%)\n",
      "Epoch: [55][100/391]\tTime 0.084 (0.087)\tData 0.001 (0.004)\tLoss 0.1689 (0.1724)\tPrec 94.531% (93.905%)\n",
      "Epoch: [55][200/391]\tTime 0.082 (0.084)\tData 0.002 (0.003)\tLoss 0.2497 (0.1813)\tPrec 93.750% (93.715%)\n",
      "Epoch: [55][300/391]\tTime 0.091 (0.084)\tData 0.002 (0.003)\tLoss 0.0873 (0.1814)\tPrec 96.875% (93.667%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.234 (0.234)\tLoss 0.2730 (0.2730)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.740% \n",
      "best acc: 87.160000\n",
      "Epoch: [56][0/391]\tTime 0.312 (0.312)\tData 0.258 (0.258)\tLoss 0.2027 (0.2027)\tPrec 92.188% (92.188%)\n",
      "Epoch: [56][100/391]\tTime 0.089 (0.088)\tData 0.002 (0.004)\tLoss 0.1953 (0.1612)\tPrec 93.750% (94.593%)\n",
      "Epoch: [56][200/391]\tTime 0.084 (0.086)\tData 0.002 (0.003)\tLoss 0.1783 (0.1723)\tPrec 92.969% (94.232%)\n",
      "Epoch: [56][300/391]\tTime 0.083 (0.085)\tData 0.002 (0.003)\tLoss 0.2307 (0.1757)\tPrec 92.969% (94.025%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.222 (0.222)\tLoss 0.2335 (0.2335)\tPrec 91.406% (91.406%)\n",
      " * Prec 86.070% \n",
      "best acc: 87.160000\n",
      "Epoch: [57][0/391]\tTime 0.325 (0.325)\tData 0.271 (0.271)\tLoss 0.1701 (0.1701)\tPrec 95.312% (95.312%)\n",
      "Epoch: [57][100/391]\tTime 0.101 (0.086)\tData 0.002 (0.004)\tLoss 0.1117 (0.1719)\tPrec 95.312% (93.982%)\n",
      "Epoch: [57][200/391]\tTime 0.081 (0.085)\tData 0.001 (0.003)\tLoss 0.2463 (0.1778)\tPrec 91.406% (93.847%)\n",
      "Epoch: [57][300/391]\tTime 0.088 (0.085)\tData 0.002 (0.003)\tLoss 0.1384 (0.1736)\tPrec 94.531% (94.077%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.198 (0.198)\tLoss 0.3165 (0.3165)\tPrec 90.625% (90.625%)\n",
      " * Prec 87.020% \n",
      "best acc: 87.160000\n",
      "Epoch: [58][0/391]\tTime 0.276 (0.276)\tData 0.217 (0.217)\tLoss 0.0873 (0.0873)\tPrec 96.875% (96.875%)\n",
      "Epoch: [58][100/391]\tTime 0.085 (0.086)\tData 0.001 (0.004)\tLoss 0.1454 (0.1604)\tPrec 96.094% (94.485%)\n",
      "Epoch: [58][200/391]\tTime 0.091 (0.086)\tData 0.001 (0.003)\tLoss 0.1078 (0.1633)\tPrec 95.312% (94.376%)\n",
      "Epoch: [58][300/391]\tTime 0.086 (0.086)\tData 0.001 (0.002)\tLoss 0.1460 (0.1675)\tPrec 94.531% (94.235%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.214 (0.214)\tLoss 0.3369 (0.3369)\tPrec 91.406% (91.406%)\n",
      " * Prec 86.580% \n",
      "best acc: 87.160000\n",
      "Epoch: [59][0/391]\tTime 0.277 (0.277)\tData 0.222 (0.222)\tLoss 0.1499 (0.1499)\tPrec 94.531% (94.531%)\n",
      "Epoch: [59][100/391]\tTime 0.085 (0.086)\tData 0.002 (0.004)\tLoss 0.1002 (0.1557)\tPrec 96.875% (94.392%)\n",
      "Epoch: [59][200/391]\tTime 0.088 (0.085)\tData 0.002 (0.003)\tLoss 0.1691 (0.1596)\tPrec 94.531% (94.356%)\n",
      "Epoch: [59][300/391]\tTime 0.085 (0.085)\tData 0.001 (0.002)\tLoss 0.1787 (0.1653)\tPrec 92.969% (94.129%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.222 (0.222)\tLoss 0.3773 (0.3773)\tPrec 86.719% (86.719%)\n",
      " * Prec 86.730% \n",
      "best acc: 87.160000\n",
      "Epoch: [60][0/391]\tTime 0.266 (0.266)\tData 0.219 (0.219)\tLoss 0.2148 (0.2148)\tPrec 91.406% (91.406%)\n",
      "Epoch: [60][100/391]\tTime 0.086 (0.085)\tData 0.002 (0.004)\tLoss 0.1294 (0.1230)\tPrec 95.312% (95.746%)\n",
      "Epoch: [60][200/391]\tTime 0.081 (0.085)\tData 0.002 (0.003)\tLoss 0.0632 (0.1114)\tPrec 98.438% (96.199%)\n",
      "Epoch: [60][300/391]\tTime 0.082 (0.085)\tData 0.002 (0.002)\tLoss 0.0733 (0.1062)\tPrec 97.656% (96.377%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.252 (0.252)\tLoss 0.2644 (0.2644)\tPrec 91.406% (91.406%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Prec 89.740% \n",
      "best acc: 89.740000\n",
      "Epoch: [61][0/391]\tTime 0.382 (0.382)\tData 0.326 (0.326)\tLoss 0.1032 (0.1032)\tPrec 96.875% (96.875%)\n",
      "Epoch: [61][100/391]\tTime 0.087 (0.089)\tData 0.002 (0.005)\tLoss 0.1212 (0.0823)\tPrec 92.969% (97.200%)\n",
      "Epoch: [61][200/391]\tTime 0.083 (0.087)\tData 0.002 (0.003)\tLoss 0.0938 (0.0836)\tPrec 96.094% (97.256%)\n",
      "Epoch: [61][300/391]\tTime 0.084 (0.086)\tData 0.002 (0.003)\tLoss 0.0575 (0.0821)\tPrec 97.656% (97.282%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.192 (0.192)\tLoss 0.1908 (0.1908)\tPrec 94.531% (94.531%)\n",
      " * Prec 89.920% \n",
      "best acc: 89.920000\n",
      "Epoch: [62][0/391]\tTime 0.311 (0.311)\tData 0.252 (0.252)\tLoss 0.0636 (0.0636)\tPrec 97.656% (97.656%)\n",
      "Epoch: [62][100/391]\tTime 0.081 (0.087)\tData 0.001 (0.004)\tLoss 0.0622 (0.0701)\tPrec 97.656% (97.726%)\n",
      "Epoch: [62][200/391]\tTime 0.083 (0.086)\tData 0.002 (0.003)\tLoss 0.0758 (0.0756)\tPrec 96.875% (97.520%)\n",
      "Epoch: [62][300/391]\tTime 0.083 (0.086)\tData 0.001 (0.002)\tLoss 0.0882 (0.0751)\tPrec 97.656% (97.498%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.217 (0.217)\tLoss 0.2207 (0.2207)\tPrec 93.750% (93.750%)\n",
      " * Prec 90.030% \n",
      "best acc: 90.030000\n",
      "Epoch: [63][0/391]\tTime 0.265 (0.265)\tData 0.215 (0.215)\tLoss 0.1108 (0.1108)\tPrec 95.312% (95.312%)\n",
      "Epoch: [63][100/391]\tTime 0.078 (0.087)\tData 0.002 (0.004)\tLoss 0.0668 (0.0713)\tPrec 98.438% (97.486%)\n",
      "Epoch: [63][200/391]\tTime 0.083 (0.086)\tData 0.002 (0.003)\tLoss 0.1139 (0.0679)\tPrec 96.094% (97.617%)\n",
      "Epoch: [63][300/391]\tTime 0.088 (0.086)\tData 0.002 (0.002)\tLoss 0.1039 (0.0685)\tPrec 96.094% (97.674%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.223 (0.223)\tLoss 0.2767 (0.2767)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.890% \n",
      "best acc: 90.030000\n",
      "Epoch: [64][0/391]\tTime 0.258 (0.258)\tData 0.197 (0.197)\tLoss 0.0223 (0.0223)\tPrec 100.000% (100.000%)\n",
      "Epoch: [64][100/391]\tTime 0.084 (0.087)\tData 0.002 (0.004)\tLoss 0.0921 (0.0614)\tPrec 97.656% (98.012%)\n",
      "Epoch: [64][200/391]\tTime 0.091 (0.086)\tData 0.002 (0.003)\tLoss 0.0845 (0.0639)\tPrec 98.438% (97.921%)\n",
      "Epoch: [64][300/391]\tTime 0.087 (0.085)\tData 0.002 (0.002)\tLoss 0.1252 (0.0626)\tPrec 95.312% (97.937%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.203 (0.203)\tLoss 0.2374 (0.2374)\tPrec 92.969% (92.969%)\n",
      " * Prec 90.190% \n",
      "best acc: 90.190000\n",
      "Epoch: [65][0/391]\tTime 0.260 (0.260)\tData 0.204 (0.204)\tLoss 0.1281 (0.1281)\tPrec 96.875% (96.875%)\n",
      "Epoch: [65][100/391]\tTime 0.078 (0.086)\tData 0.001 (0.004)\tLoss 0.0550 (0.0551)\tPrec 96.875% (98.028%)\n",
      "Epoch: [65][200/391]\tTime 0.083 (0.085)\tData 0.002 (0.003)\tLoss 0.0596 (0.0572)\tPrec 98.438% (98.018%)\n",
      "Epoch: [65][300/391]\tTime 0.087 (0.084)\tData 0.001 (0.002)\tLoss 0.0709 (0.0583)\tPrec 98.438% (97.999%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.208 (0.208)\tLoss 0.2432 (0.2432)\tPrec 93.750% (93.750%)\n",
      " * Prec 90.320% \n",
      "best acc: 90.320000\n",
      "Epoch: [66][0/391]\tTime 0.257 (0.257)\tData 0.194 (0.194)\tLoss 0.0366 (0.0366)\tPrec 99.219% (99.219%)\n",
      "Epoch: [66][100/391]\tTime 0.079 (0.087)\tData 0.001 (0.004)\tLoss 0.0628 (0.0557)\tPrec 97.656% (98.051%)\n",
      "Epoch: [66][200/391]\tTime 0.085 (0.086)\tData 0.002 (0.003)\tLoss 0.0366 (0.0568)\tPrec 99.219% (98.084%)\n",
      "Epoch: [66][300/391]\tTime 0.085 (0.085)\tData 0.002 (0.002)\tLoss 0.1543 (0.0571)\tPrec 95.312% (98.030%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.197 (0.197)\tLoss 0.2487 (0.2487)\tPrec 92.969% (92.969%)\n",
      " * Prec 90.230% \n",
      "best acc: 90.320000\n",
      "Epoch: [67][0/391]\tTime 0.327 (0.327)\tData 0.262 (0.262)\tLoss 0.0245 (0.0245)\tPrec 99.219% (99.219%)\n",
      "Epoch: [67][100/391]\tTime 0.083 (0.087)\tData 0.002 (0.004)\tLoss 0.0596 (0.0498)\tPrec 97.656% (98.345%)\n",
      "Epoch: [67][200/391]\tTime 0.087 (0.085)\tData 0.003 (0.003)\tLoss 0.0697 (0.0531)\tPrec 96.094% (98.173%)\n",
      "Epoch: [67][300/391]\tTime 0.083 (0.085)\tData 0.002 (0.003)\tLoss 0.1739 (0.0548)\tPrec 92.969% (98.105%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.260 (0.260)\tLoss 0.2932 (0.2932)\tPrec 90.625% (90.625%)\n",
      " * Prec 90.140% \n",
      "best acc: 90.320000\n",
      "Epoch: [68][0/391]\tTime 0.267 (0.267)\tData 0.210 (0.210)\tLoss 0.0750 (0.0750)\tPrec 98.438% (98.438%)\n",
      "Epoch: [68][100/391]\tTime 0.085 (0.087)\tData 0.001 (0.004)\tLoss 0.0912 (0.0481)\tPrec 96.094% (98.407%)\n",
      "Epoch: [68][200/391]\tTime 0.087 (0.085)\tData 0.002 (0.003)\tLoss 0.0345 (0.0487)\tPrec 99.219% (98.321%)\n",
      "Epoch: [68][300/391]\tTime 0.081 (0.085)\tData 0.002 (0.002)\tLoss 0.0598 (0.0517)\tPrec 97.656% (98.269%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.206 (0.206)\tLoss 0.2167 (0.2167)\tPrec 92.188% (92.188%)\n",
      " * Prec 90.320% \n",
      "best acc: 90.320000\n",
      "Epoch: [69][0/391]\tTime 0.270 (0.270)\tData 0.207 (0.207)\tLoss 0.0402 (0.0402)\tPrec 98.438% (98.438%)\n",
      "Epoch: [69][100/391]\tTime 0.083 (0.086)\tData 0.002 (0.004)\tLoss 0.0168 (0.0467)\tPrec 99.219% (98.445%)\n",
      "Epoch: [69][200/391]\tTime 0.085 (0.085)\tData 0.002 (0.003)\tLoss 0.0418 (0.0449)\tPrec 98.438% (98.492%)\n",
      "Epoch: [69][300/391]\tTime 0.084 (0.085)\tData 0.001 (0.002)\tLoss 0.0356 (0.0461)\tPrec 98.438% (98.419%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.217 (0.217)\tLoss 0.2400 (0.2400)\tPrec 93.750% (93.750%)\n",
      " * Prec 90.170% \n",
      "best acc: 90.320000\n",
      "Epoch: [70][0/391]\tTime 0.297 (0.297)\tData 0.237 (0.237)\tLoss 0.1162 (0.1162)\tPrec 97.656% (97.656%)\n",
      "Epoch: [70][100/391]\tTime 0.057 (0.086)\tData 0.002 (0.004)\tLoss 0.0809 (0.0477)\tPrec 97.656% (98.368%)\n",
      "Epoch: [70][200/391]\tTime 0.083 (0.085)\tData 0.002 (0.003)\tLoss 0.0158 (0.0476)\tPrec 99.219% (98.403%)\n",
      "Epoch: [70][300/391]\tTime 0.082 (0.085)\tData 0.002 (0.003)\tLoss 0.0632 (0.0483)\tPrec 96.875% (98.409%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.213 (0.213)\tLoss 0.2203 (0.2203)\tPrec 92.188% (92.188%)\n",
      " * Prec 90.280% \n",
      "best acc: 90.320000\n",
      "Epoch: [71][0/391]\tTime 0.252 (0.252)\tData 0.196 (0.196)\tLoss 0.0408 (0.0408)\tPrec 98.438% (98.438%)\n",
      "Epoch: [71][100/391]\tTime 0.084 (0.085)\tData 0.002 (0.004)\tLoss 0.0838 (0.0436)\tPrec 95.312% (98.422%)\n",
      "Epoch: [71][200/391]\tTime 0.087 (0.085)\tData 0.002 (0.003)\tLoss 0.0175 (0.0432)\tPrec 100.000% (98.438%)\n",
      "Epoch: [71][300/391]\tTime 0.086 (0.085)\tData 0.002 (0.002)\tLoss 0.0518 (0.0455)\tPrec 97.656% (98.383%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.221 (0.221)\tLoss 0.3480 (0.3480)\tPrec 91.406% (91.406%)\n",
      " * Prec 90.110% \n",
      "best acc: 90.320000\n",
      "Epoch: [72][0/391]\tTime 0.239 (0.239)\tData 0.183 (0.183)\tLoss 0.1151 (0.1151)\tPrec 96.094% (96.094%)\n",
      "Epoch: [72][100/391]\tTime 0.083 (0.086)\tData 0.002 (0.004)\tLoss 0.0951 (0.0467)\tPrec 96.875% (98.360%)\n",
      "Epoch: [72][200/391]\tTime 0.083 (0.086)\tData 0.002 (0.003)\tLoss 0.0630 (0.0468)\tPrec 97.656% (98.344%)\n",
      "Epoch: [72][300/391]\tTime 0.085 (0.085)\tData 0.002 (0.002)\tLoss 0.0246 (0.0468)\tPrec 100.000% (98.375%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.219 (0.219)\tLoss 0.3025 (0.3025)\tPrec 90.625% (90.625%)\n",
      " * Prec 90.070% \n",
      "best acc: 90.320000\n",
      "Epoch: [73][0/391]\tTime 0.281 (0.281)\tData 0.238 (0.238)\tLoss 0.0349 (0.0349)\tPrec 98.438% (98.438%)\n",
      "Epoch: [73][100/391]\tTime 0.086 (0.086)\tData 0.002 (0.004)\tLoss 0.0392 (0.0400)\tPrec 98.438% (98.554%)\n",
      "Epoch: [73][200/391]\tTime 0.086 (0.086)\tData 0.002 (0.003)\tLoss 0.0178 (0.0447)\tPrec 99.219% (98.438%)\n",
      "Epoch: [73][300/391]\tTime 0.087 (0.085)\tData 0.001 (0.002)\tLoss 0.0293 (0.0447)\tPrec 100.000% (98.448%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.219 (0.219)\tLoss 0.2425 (0.2425)\tPrec 94.531% (94.531%)\n",
      " * Prec 90.020% \n",
      "best acc: 90.320000\n",
      "Epoch: [74][0/391]\tTime 0.395 (0.395)\tData 0.326 (0.326)\tLoss 0.0420 (0.0420)\tPrec 98.438% (98.438%)\n",
      "Epoch: [74][100/391]\tTime 0.084 (0.088)\tData 0.001 (0.005)\tLoss 0.0732 (0.0404)\tPrec 97.656% (98.623%)\n",
      "Epoch: [74][200/391]\tTime 0.084 (0.086)\tData 0.002 (0.003)\tLoss 0.0163 (0.0439)\tPrec 100.000% (98.535%)\n",
      "Epoch: [74][300/391]\tTime 0.078 (0.086)\tData 0.002 (0.003)\tLoss 0.0306 (0.0443)\tPrec 98.438% (98.484%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.212 (0.212)\tLoss 0.2496 (0.2496)\tPrec 94.531% (94.531%)\n",
      " * Prec 90.080% \n",
      "best acc: 90.320000\n",
      "Epoch: [75][0/391]\tTime 0.267 (0.267)\tData 0.209 (0.209)\tLoss 0.0444 (0.0444)\tPrec 98.438% (98.438%)\n",
      "Epoch: [75][100/391]\tTime 0.092 (0.086)\tData 0.002 (0.004)\tLoss 0.0597 (0.0407)\tPrec 99.219% (98.523%)\n",
      "Epoch: [75][200/391]\tTime 0.085 (0.085)\tData 0.002 (0.003)\tLoss 0.0124 (0.0383)\tPrec 100.000% (98.686%)\n",
      "Epoch: [75][300/391]\tTime 0.089 (0.086)\tData 0.001 (0.002)\tLoss 0.0579 (0.0404)\tPrec 97.656% (98.614%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.227 (0.227)\tLoss 0.2113 (0.2113)\tPrec 92.969% (92.969%)\n",
      " * Prec 90.230% \n",
      "best acc: 90.320000\n",
      "Epoch: [76][0/391]\tTime 0.264 (0.264)\tData 0.203 (0.203)\tLoss 0.0345 (0.0345)\tPrec 98.438% (98.438%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [76][100/391]\tTime 0.082 (0.087)\tData 0.002 (0.004)\tLoss 0.0505 (0.0362)\tPrec 97.656% (98.708%)\n",
      "Epoch: [76][200/391]\tTime 0.088 (0.085)\tData 0.001 (0.003)\tLoss 0.0440 (0.0388)\tPrec 98.438% (98.644%)\n",
      "Epoch: [76][300/391]\tTime 0.085 (0.085)\tData 0.001 (0.002)\tLoss 0.0112 (0.0387)\tPrec 100.000% (98.624%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.229 (0.229)\tLoss 0.2904 (0.2904)\tPrec 91.406% (91.406%)\n",
      " * Prec 90.140% \n",
      "best acc: 90.320000\n",
      "Epoch: [77][0/391]\tTime 0.270 (0.270)\tData 0.206 (0.206)\tLoss 0.0074 (0.0074)\tPrec 100.000% (100.000%)\n",
      "Epoch: [77][100/391]\tTime 0.086 (0.087)\tData 0.002 (0.004)\tLoss 0.0686 (0.0370)\tPrec 96.875% (98.801%)\n",
      "Epoch: [77][200/391]\tTime 0.085 (0.086)\tData 0.002 (0.003)\tLoss 0.0173 (0.0374)\tPrec 99.219% (98.795%)\n",
      "Epoch: [77][300/391]\tTime 0.084 (0.085)\tData 0.002 (0.002)\tLoss 0.0822 (0.0363)\tPrec 98.438% (98.785%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.211 (0.211)\tLoss 0.2566 (0.2566)\tPrec 94.531% (94.531%)\n",
      " * Prec 90.270% \n",
      "best acc: 90.320000\n",
      "Epoch: [78][0/391]\tTime 0.278 (0.278)\tData 0.213 (0.213)\tLoss 0.0929 (0.0929)\tPrec 97.656% (97.656%)\n",
      "Epoch: [78][100/391]\tTime 0.087 (0.086)\tData 0.002 (0.004)\tLoss 0.0845 (0.0359)\tPrec 98.438% (98.700%)\n",
      "Epoch: [78][200/391]\tTime 0.088 (0.086)\tData 0.002 (0.003)\tLoss 0.0302 (0.0370)\tPrec 99.219% (98.717%)\n",
      "Epoch: [78][300/391]\tTime 0.096 (0.085)\tData 0.001 (0.002)\tLoss 0.0652 (0.0365)\tPrec 98.438% (98.728%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.226 (0.226)\tLoss 0.2699 (0.2699)\tPrec 91.406% (91.406%)\n",
      " * Prec 90.070% \n",
      "best acc: 90.320000\n",
      "Epoch: [79][0/391]\tTime 0.292 (0.292)\tData 0.235 (0.235)\tLoss 0.0166 (0.0166)\tPrec 99.219% (99.219%)\n",
      "Epoch: [79][100/391]\tTime 0.083 (0.086)\tData 0.002 (0.004)\tLoss 0.0162 (0.0347)\tPrec 99.219% (98.762%)\n",
      "Epoch: [79][200/391]\tTime 0.083 (0.085)\tData 0.002 (0.003)\tLoss 0.0355 (0.0352)\tPrec 99.219% (98.725%)\n",
      "Epoch: [79][300/391]\tTime 0.085 (0.085)\tData 0.001 (0.003)\tLoss 0.0223 (0.0346)\tPrec 99.219% (98.757%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.234 (0.234)\tLoss 0.2710 (0.2710)\tPrec 92.188% (92.188%)\n",
      " * Prec 90.410% \n",
      "best acc: 90.410000\n",
      "Epoch: [80][0/391]\tTime 0.273 (0.273)\tData 0.210 (0.210)\tLoss 0.0228 (0.0228)\tPrec 99.219% (99.219%)\n",
      "Epoch: [80][100/391]\tTime 0.081 (0.086)\tData 0.002 (0.004)\tLoss 0.0115 (0.0310)\tPrec 100.000% (98.871%)\n",
      "Epoch: [80][200/391]\tTime 0.082 (0.085)\tData 0.002 (0.003)\tLoss 0.0290 (0.0326)\tPrec 98.438% (98.884%)\n",
      "Epoch: [80][300/391]\tTime 0.084 (0.085)\tData 0.002 (0.002)\tLoss 0.0138 (0.0329)\tPrec 100.000% (98.868%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.225 (0.225)\tLoss 0.2549 (0.2549)\tPrec 92.969% (92.969%)\n",
      " * Prec 90.270% \n",
      "best acc: 90.410000\n",
      "Epoch: [81][0/391]\tTime 0.305 (0.305)\tData 0.255 (0.255)\tLoss 0.0387 (0.0387)\tPrec 98.438% (98.438%)\n",
      "Epoch: [81][100/391]\tTime 0.088 (0.090)\tData 0.002 (0.004)\tLoss 0.0239 (0.0337)\tPrec 98.438% (98.786%)\n",
      "Epoch: [81][200/391]\tTime 0.100 (0.089)\tData 0.001 (0.003)\tLoss 0.0118 (0.0345)\tPrec 100.000% (98.826%)\n",
      "Epoch: [81][300/391]\tTime 0.084 (0.087)\tData 0.002 (0.003)\tLoss 0.0043 (0.0332)\tPrec 100.000% (98.881%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.186 (0.186)\tLoss 0.2302 (0.2302)\tPrec 93.750% (93.750%)\n",
      " * Prec 90.630% \n",
      "best acc: 90.630000\n",
      "Epoch: [82][0/391]\tTime 0.261 (0.261)\tData 0.199 (0.199)\tLoss 0.0085 (0.0085)\tPrec 100.000% (100.000%)\n",
      "Epoch: [82][100/391]\tTime 0.082 (0.089)\tData 0.002 (0.004)\tLoss 0.0084 (0.0299)\tPrec 100.000% (98.925%)\n",
      "Epoch: [82][200/391]\tTime 0.090 (0.087)\tData 0.002 (0.003)\tLoss 0.0206 (0.0306)\tPrec 99.219% (98.954%)\n",
      "Epoch: [82][300/391]\tTime 0.089 (0.087)\tData 0.002 (0.002)\tLoss 0.0186 (0.0320)\tPrec 100.000% (98.889%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.206 (0.206)\tLoss 0.2792 (0.2792)\tPrec 93.750% (93.750%)\n",
      " * Prec 90.320% \n",
      "best acc: 90.630000\n",
      "Epoch: [83][0/391]\tTime 0.253 (0.253)\tData 0.195 (0.195)\tLoss 0.0309 (0.0309)\tPrec 99.219% (99.219%)\n",
      "Epoch: [83][100/391]\tTime 0.053 (0.086)\tData 0.002 (0.004)\tLoss 0.0452 (0.0323)\tPrec 97.656% (98.933%)\n",
      "Epoch: [83][200/391]\tTime 0.087 (0.087)\tData 0.004 (0.003)\tLoss 0.0107 (0.0311)\tPrec 100.000% (98.958%)\n",
      "Epoch: [83][300/391]\tTime 0.087 (0.086)\tData 0.001 (0.002)\tLoss 0.0429 (0.0311)\tPrec 96.875% (98.944%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.252 (0.252)\tLoss 0.3212 (0.3212)\tPrec 92.188% (92.188%)\n",
      " * Prec 90.270% \n",
      "best acc: 90.630000\n",
      "Epoch: [84][0/391]\tTime 0.253 (0.253)\tData 0.202 (0.202)\tLoss 0.0138 (0.0138)\tPrec 99.219% (99.219%)\n",
      "Epoch: [84][100/391]\tTime 0.082 (0.086)\tData 0.002 (0.004)\tLoss 0.0740 (0.0318)\tPrec 97.656% (98.925%)\n",
      "Epoch: [84][200/391]\tTime 0.083 (0.086)\tData 0.002 (0.003)\tLoss 0.0414 (0.0320)\tPrec 98.438% (98.877%)\n",
      "Epoch: [84][300/391]\tTime 0.099 (0.086)\tData 0.001 (0.002)\tLoss 0.0144 (0.0325)\tPrec 99.219% (98.879%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.190 (0.190)\tLoss 0.2530 (0.2530)\tPrec 93.750% (93.750%)\n",
      " * Prec 90.230% \n",
      "best acc: 90.630000\n",
      "Epoch: [85][0/391]\tTime 0.252 (0.252)\tData 0.196 (0.196)\tLoss 0.0331 (0.0331)\tPrec 98.438% (98.438%)\n",
      "Epoch: [85][100/391]\tTime 0.087 (0.087)\tData 0.001 (0.004)\tLoss 0.0287 (0.0312)\tPrec 98.438% (98.933%)\n",
      "Epoch: [85][200/391]\tTime 0.090 (0.087)\tData 0.002 (0.003)\tLoss 0.0431 (0.0302)\tPrec 97.656% (98.919%)\n",
      "Epoch: [85][300/391]\tTime 0.085 (0.087)\tData 0.002 (0.002)\tLoss 0.0314 (0.0309)\tPrec 98.438% (98.905%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.194 (0.194)\tLoss 0.2681 (0.2681)\tPrec 91.406% (91.406%)\n",
      " * Prec 90.240% \n",
      "best acc: 90.630000\n",
      "Epoch: [86][0/391]\tTime 0.278 (0.278)\tData 0.224 (0.224)\tLoss 0.0562 (0.0562)\tPrec 96.875% (96.875%)\n",
      "Epoch: [86][100/391]\tTime 0.088 (0.090)\tData 0.001 (0.004)\tLoss 0.0188 (0.0297)\tPrec 99.219% (98.925%)\n",
      "Epoch: [86][200/391]\tTime 0.084 (0.088)\tData 0.002 (0.003)\tLoss 0.0193 (0.0306)\tPrec 100.000% (98.958%)\n",
      "Epoch: [86][300/391]\tTime 0.086 (0.087)\tData 0.001 (0.002)\tLoss 0.0427 (0.0312)\tPrec 98.438% (98.918%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.214 (0.214)\tLoss 0.3141 (0.3141)\tPrec 92.969% (92.969%)\n",
      " * Prec 90.320% \n",
      "best acc: 90.630000\n",
      "Epoch: [87][0/391]\tTime 0.215 (0.215)\tData 0.170 (0.170)\tLoss 0.0422 (0.0422)\tPrec 97.656% (97.656%)\n",
      "Epoch: [87][100/391]\tTime 0.085 (0.087)\tData 0.002 (0.003)\tLoss 0.0065 (0.0295)\tPrec 100.000% (98.987%)\n",
      "Epoch: [87][200/391]\tTime 0.084 (0.086)\tData 0.002 (0.003)\tLoss 0.0202 (0.0287)\tPrec 99.219% (99.071%)\n",
      "Epoch: [87][300/391]\tTime 0.051 (0.086)\tData 0.001 (0.002)\tLoss 0.0072 (0.0280)\tPrec 100.000% (99.089%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.239 (0.239)\tLoss 0.2899 (0.2899)\tPrec 92.969% (92.969%)\n",
      " * Prec 90.330% \n",
      "best acc: 90.630000\n",
      "Epoch: [88][0/391]\tTime 0.267 (0.267)\tData 0.214 (0.214)\tLoss 0.0932 (0.0932)\tPrec 96.094% (96.094%)\n",
      "Epoch: [88][100/391]\tTime 0.082 (0.088)\tData 0.002 (0.004)\tLoss 0.0306 (0.0305)\tPrec 98.438% (98.940%)\n",
      "Epoch: [88][200/391]\tTime 0.086 (0.087)\tData 0.002 (0.003)\tLoss 0.0115 (0.0298)\tPrec 100.000% (98.943%)\n",
      "Epoch: [88][300/391]\tTime 0.085 (0.085)\tData 0.002 (0.002)\tLoss 0.0110 (0.0302)\tPrec 100.000% (98.967%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.206 (0.206)\tLoss 0.3248 (0.3248)\tPrec 93.750% (93.750%)\n",
      " * Prec 90.210% \n",
      "best acc: 90.630000\n",
      "Epoch: [89][0/391]\tTime 0.252 (0.252)\tData 0.195 (0.195)\tLoss 0.0213 (0.0213)\tPrec 99.219% (99.219%)\n",
      "Epoch: [89][100/391]\tTime 0.085 (0.086)\tData 0.001 (0.004)\tLoss 0.0143 (0.0288)\tPrec 100.000% (99.056%)\n",
      "Epoch: [89][200/391]\tTime 0.086 (0.085)\tData 0.002 (0.003)\tLoss 0.0525 (0.0299)\tPrec 98.438% (99.017%)\n",
      "Epoch: [89][300/391]\tTime 0.087 (0.085)\tData 0.002 (0.002)\tLoss 0.0024 (0.0287)\tPrec 100.000% (99.019%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.244 (0.244)\tLoss 0.2329 (0.2329)\tPrec 92.969% (92.969%)\n",
      " * Prec 90.340% \n",
      "best acc: 90.630000\n",
      "Epoch: [90][0/391]\tTime 0.316 (0.316)\tData 0.259 (0.259)\tLoss 0.0220 (0.0220)\tPrec 99.219% (99.219%)\n",
      "Epoch: [90][100/391]\tTime 0.083 (0.087)\tData 0.003 (0.004)\tLoss 0.0208 (0.0283)\tPrec 99.219% (99.064%)\n",
      "Epoch: [90][200/391]\tTime 0.071 (0.086)\tData 0.001 (0.003)\tLoss 0.0152 (0.0294)\tPrec 100.000% (98.993%)\n",
      "Epoch: [90][300/391]\tTime 0.080 (0.086)\tData 0.002 (0.003)\tLoss 0.0189 (0.0294)\tPrec 99.219% (98.996%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.235 (0.235)\tLoss 0.3423 (0.3423)\tPrec 92.188% (92.188%)\n",
      " * Prec 90.460% \n",
      "best acc: 90.630000\n",
      "Epoch: [91][0/391]\tTime 0.258 (0.258)\tData 0.190 (0.190)\tLoss 0.0256 (0.0256)\tPrec 99.219% (99.219%)\n",
      "Epoch: [91][100/391]\tTime 0.092 (0.088)\tData 0.002 (0.004)\tLoss 0.0307 (0.0308)\tPrec 99.219% (98.925%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [91][200/391]\tTime 0.091 (0.086)\tData 0.002 (0.003)\tLoss 0.0268 (0.0284)\tPrec 99.219% (99.024%)\n",
      "Epoch: [91][300/391]\tTime 0.085 (0.085)\tData 0.002 (0.003)\tLoss 0.0310 (0.0280)\tPrec 98.438% (99.068%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.248 (0.248)\tLoss 0.3416 (0.3416)\tPrec 92.969% (92.969%)\n",
      " * Prec 90.410% \n",
      "best acc: 90.630000\n",
      "Epoch: [92][0/391]\tTime 0.281 (0.281)\tData 0.223 (0.223)\tLoss 0.0077 (0.0077)\tPrec 100.000% (100.000%)\n",
      "Epoch: [92][100/391]\tTime 0.088 (0.087)\tData 0.002 (0.004)\tLoss 0.0732 (0.0275)\tPrec 96.875% (99.002%)\n",
      "Epoch: [92][200/391]\tTime 0.097 (0.087)\tData 0.002 (0.003)\tLoss 0.0237 (0.0268)\tPrec 99.219% (99.052%)\n",
      "Epoch: [92][300/391]\tTime 0.086 (0.087)\tData 0.002 (0.003)\tLoss 0.0730 (0.0265)\tPrec 98.438% (99.073%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.217 (0.217)\tLoss 0.2975 (0.2975)\tPrec 91.406% (91.406%)\n",
      " * Prec 90.350% \n",
      "best acc: 90.630000\n",
      "Epoch: [93][0/391]\tTime 0.273 (0.273)\tData 0.214 (0.214)\tLoss 0.0247 (0.0247)\tPrec 99.219% (99.219%)\n",
      "Epoch: [93][100/391]\tTime 0.084 (0.086)\tData 0.001 (0.004)\tLoss 0.0045 (0.0277)\tPrec 100.000% (99.049%)\n",
      "Epoch: [93][200/391]\tTime 0.088 (0.086)\tData 0.002 (0.003)\tLoss 0.0142 (0.0280)\tPrec 100.000% (99.056%)\n",
      "Epoch: [93][300/391]\tTime 0.082 (0.086)\tData 0.002 (0.003)\tLoss 0.0339 (0.0275)\tPrec 97.656% (99.084%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.257 (0.257)\tLoss 0.2701 (0.2701)\tPrec 93.750% (93.750%)\n",
      " * Prec 90.440% \n",
      "best acc: 90.630000\n",
      "Epoch: [94][0/391]\tTime 0.287 (0.287)\tData 0.221 (0.221)\tLoss 0.0192 (0.0192)\tPrec 99.219% (99.219%)\n",
      "Epoch: [94][100/391]\tTime 0.087 (0.085)\tData 0.002 (0.004)\tLoss 0.0322 (0.0265)\tPrec 97.656% (99.072%)\n",
      "Epoch: [94][200/391]\tTime 0.082 (0.085)\tData 0.002 (0.003)\tLoss 0.0246 (0.0291)\tPrec 99.219% (98.974%)\n",
      "Epoch: [94][300/391]\tTime 0.090 (0.085)\tData 0.003 (0.003)\tLoss 0.0515 (0.0281)\tPrec 97.656% (99.016%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.220 (0.220)\tLoss 0.3040 (0.3040)\tPrec 92.969% (92.969%)\n",
      " * Prec 90.330% \n",
      "best acc: 90.630000\n",
      "Epoch: [95][0/391]\tTime 0.329 (0.329)\tData 0.262 (0.262)\tLoss 0.0049 (0.0049)\tPrec 100.000% (100.000%)\n",
      "Epoch: [95][100/391]\tTime 0.085 (0.087)\tData 0.002 (0.004)\tLoss 0.0301 (0.0278)\tPrec 98.438% (98.987%)\n",
      "Epoch: [95][200/391]\tTime 0.086 (0.087)\tData 0.002 (0.003)\tLoss 0.0181 (0.0264)\tPrec 99.219% (99.052%)\n",
      "Epoch: [95][300/391]\tTime 0.082 (0.086)\tData 0.002 (0.003)\tLoss 0.0072 (0.0269)\tPrec 100.000% (99.058%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.209 (0.209)\tLoss 0.2925 (0.2925)\tPrec 92.188% (92.188%)\n",
      " * Prec 90.420% \n",
      "best acc: 90.630000\n",
      "Epoch: [96][0/391]\tTime 0.300 (0.300)\tData 0.239 (0.239)\tLoss 0.0654 (0.0654)\tPrec 97.656% (97.656%)\n",
      "Epoch: [96][100/391]\tTime 0.085 (0.089)\tData 0.003 (0.004)\tLoss 0.0768 (0.0276)\tPrec 97.656% (99.049%)\n",
      "Epoch: [96][200/391]\tTime 0.082 (0.087)\tData 0.002 (0.003)\tLoss 0.0398 (0.0280)\tPrec 98.438% (99.028%)\n",
      "Epoch: [96][300/391]\tTime 0.085 (0.087)\tData 0.002 (0.003)\tLoss 0.0836 (0.0271)\tPrec 97.656% (99.089%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.213 (0.213)\tLoss 0.3079 (0.3079)\tPrec 92.969% (92.969%)\n",
      " * Prec 90.450% \n",
      "best acc: 90.630000\n",
      "Epoch: [97][0/391]\tTime 0.310 (0.310)\tData 0.256 (0.256)\tLoss 0.0271 (0.0271)\tPrec 99.219% (99.219%)\n",
      "Epoch: [97][100/391]\tTime 0.090 (0.089)\tData 0.002 (0.004)\tLoss 0.0195 (0.0292)\tPrec 99.219% (99.002%)\n",
      "Epoch: [97][200/391]\tTime 0.087 (0.088)\tData 0.002 (0.003)\tLoss 0.0063 (0.0286)\tPrec 100.000% (99.009%)\n",
      "Epoch: [97][300/391]\tTime 0.079 (0.087)\tData 0.002 (0.003)\tLoss 0.0222 (0.0277)\tPrec 99.219% (99.053%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.192 (0.192)\tLoss 0.3301 (0.3301)\tPrec 92.188% (92.188%)\n",
      " * Prec 90.330% \n",
      "best acc: 90.630000\n",
      "Epoch: [98][0/391]\tTime 0.297 (0.297)\tData 0.240 (0.240)\tLoss 0.0148 (0.0148)\tPrec 99.219% (99.219%)\n",
      "Epoch: [98][100/391]\tTime 0.089 (0.088)\tData 0.002 (0.004)\tLoss 0.0060 (0.0260)\tPrec 100.000% (99.118%)\n",
      "Epoch: [98][200/391]\tTime 0.086 (0.087)\tData 0.002 (0.003)\tLoss 0.0444 (0.0245)\tPrec 98.438% (99.192%)\n",
      "Epoch: [98][300/391]\tTime 0.100 (0.086)\tData 0.002 (0.003)\tLoss 0.0268 (0.0253)\tPrec 98.438% (99.167%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.238 (0.238)\tLoss 0.3132 (0.3132)\tPrec 92.188% (92.188%)\n",
      " * Prec 90.380% \n",
      "best acc: 90.630000\n",
      "Epoch: [99][0/391]\tTime 0.295 (0.295)\tData 0.244 (0.244)\tLoss 0.0407 (0.0407)\tPrec 97.656% (97.656%)\n",
      "Epoch: [99][100/391]\tTime 0.085 (0.089)\tData 0.002 (0.004)\tLoss 0.0024 (0.0235)\tPrec 100.000% (99.304%)\n",
      "Epoch: [99][200/391]\tTime 0.082 (0.087)\tData 0.002 (0.003)\tLoss 0.0477 (0.0249)\tPrec 98.438% (99.227%)\n",
      "Epoch: [99][300/391]\tTime 0.083 (0.086)\tData 0.002 (0.003)\tLoss 0.0063 (0.0247)\tPrec 100.000% (99.180%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.266 (0.266)\tLoss 0.2783 (0.2783)\tPrec 93.750% (93.750%)\n",
      " * Prec 90.190% \n",
      "best acc: 90.630000\n"
     ]
    }
   ],
   "source": [
    "# This cell won't be given, but students will complete the training\n",
    "\n",
    "lr = 4e-2\n",
    "weight_decay = 1e-4\n",
    "epochs = 100\n",
    "best_prec = 0\n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "fdir = 'result/'+str(model_name)\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "        \n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # evaluate on test set\n",
    "    print(\"Validation starts\")\n",
    "    prec = validate(testloader, model, criterion)\n",
    "\n",
    "    # remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec,best_prec)\n",
    "    print('best acc: {:1f}'.format(best_prec))\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b081a0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prehooked\n",
      "QuantConv2d(\n",
      "  3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 1\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 2\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 3\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 4\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 5\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 6\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 7\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 8\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 9\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  8, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 10\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 11\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 12\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 13\n"
     ]
    }
   ],
   "source": [
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []  \n",
    "        \n",
    "######### Save inputs from selected layer ##########\n",
    "save_output = SaveOutput()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\") \n",
    "counter =0\n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        print(\"prehooked\")\n",
    "        counter += 1\n",
    "        print(layer, counter)\n",
    "        layer.register_forward_pre_hook(save_output)       ## Input for the module will be grapped       \n",
    "####################################################\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.to(device)\n",
    "out = model(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW\n",
    "\n",
    "#  1. Train with 4 bits for both weight and activation to achieve >90% accuracy\n",
    "#  2. Find x_int and w_int for the 2nd convolution layer\n",
    "#  3. Check the recovered psum has similar value to the un-quantized original psum\n",
    "#     (such as example 1 in W3S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "entertaining-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 9062/10000 (91%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = \"result/VGG16_quant4bit/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7138a0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(save_output.outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(save_output.outputs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "spoken-worst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 7.0000, -7.0000,  7.0000],\n",
      "          [-2.0000,  7.0000, -7.0000],\n",
      "          [-7.0000,  7.0000, -7.0000]],\n",
      "\n",
      "         [[ 7.0000, -7.0000, -7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[ 7.0000, -7.0000,  7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [ 7.0000, -7.0000,  7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000, -7.0000],\n",
      "          [ 7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000, -7.0000],\n",
      "          [ 7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000, -7.0000],\n",
      "          [ 7.0000,  7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000,  7.0000]],\n",
      "\n",
      "         [[-7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 7.0000,  7.0000,  7.0000],\n",
      "          [ 1.0000,  7.0000,  7.0000],\n",
      "          [-7.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[-7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000, -5.0000],\n",
      "          [-7.0000, -7.0000, -2.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000, -7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000,  7.0000],\n",
      "          [-7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000, -7.0000,  7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000, -7.0000],\n",
      "          [ 7.0000, -7.0000, -7.0000],\n",
      "          [ 7.0000, -7.0000, -7.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 7.0000,  7.0000, -7.0000],\n",
      "          [ 7.0000,  7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000,  7.0000]],\n",
      "\n",
      "         [[ 7.0000, -7.0000, -7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[ 7.0000, -7.0000,  7.0000],\n",
      "          [ 7.0000, -7.0000, -7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000,  7.0000],\n",
      "          [-7.0000, -7.0000,  7.0000],\n",
      "          [-7.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[-7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [ 7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000, -7.0000],\n",
      "          [ 7.0000, -7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[-7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000, -7.0000,  7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000, -7.0000,  7.0000]],\n",
      "\n",
      "         [[ 7.0000, -7.0000, -7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000, -7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [ 7.0000,  7.0000, -7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000, -7.0000],\n",
      "          [ 7.0000,  7.0000, -7.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000, -7.0000],\n",
      "          [ 7.0000,  7.0000, -7.0000]],\n",
      "\n",
      "         [[ 7.0000, -7.0000, -7.0000],\n",
      "          [ 7.0000,  7.0000, -7.0000],\n",
      "          [ 7.0000,  7.0000, -7.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000,  7.0000],\n",
      "          [-7.0000, -7.0000,  7.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 7.0000,  7.0000, -7.0000],\n",
      "          [ 7.0000,  7.0000, -7.0000],\n",
      "          [ 7.0000,  7.0000, -7.0000]],\n",
      "\n",
      "         [[-7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[ 7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[-7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000, -7.0000,  7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[ 7.0000, -7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[-7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000,  7.0000, -7.0000]],\n",
      "\n",
      "         [[-7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000,  7.0000, -7.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[-7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000, -7.0000,  7.0000],\n",
      "          [-7.0000,  7.0000, -7.0000]],\n",
      "\n",
      "         [[ 7.0000, -7.0000, -7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000, -7.0000,  7.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[-7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000, -7.0000,  7.0000],\n",
      "          [-1.0000, -7.0000,  7.0000]],\n",
      "\n",
      "         [[ 7.0000, -7.0000, -7.0000],\n",
      "          [ 7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [ 7.0000, -7.0000,  4.0000]],\n",
      "\n",
      "         [[-7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [ 7.0000,  1.0000, -7.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000, -7.0000,  7.0000]],\n",
      "\n",
      "         [[-7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000, -7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000, -7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[ 7.0000, -7.0000, -7.0000],\n",
      "          [ 7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000,  7.0000, -7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [ 7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [ 7.0000,  7.0000, -7.0000]],\n",
      "\n",
      "         [[-7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000, -7.0000]]],\n",
      "\n",
      "\n",
      "        [[[-7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[-7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000, -7.0000,  7.0000],\n",
      "          [-7.0000,  7.0000, -7.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000, -7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000,  7.0000, -7.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000, -7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[-7.0000,  7.0000, -7.0000],\n",
      "          [-7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]]]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "w_bit = 4\n",
    "weight_q = model.features[27].weight_q # quantized value is stored during the training\n",
    "w_alpha = model.features[27].weight_quant.wgt_alpha\n",
    "w_delta = w_alpha/(2**(w_bit-1)-1)\n",
    "weight_int = weight_q/w_delta\n",
    "print(weight_int) # you should see clean integer numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "interior-oxygen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 5.0000,  2.0000,  2.0000,  1.0000],\n",
      "          [ 5.0000,  0.0000,  2.0000,  0.0000],\n",
      "          [ 4.0000,  1.0000,  4.0000,  2.0000],\n",
      "          [ 4.0000,  2.0000,  4.0000,  3.0000]],\n",
      "\n",
      "         [[ 2.0000,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  1.0000],\n",
      "          [ 2.0000,  3.0000,  4.0000,  3.0000],\n",
      "          [ 2.0000,  3.0000,  4.0000,  3.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 1.0000,  1.0000,  0.0000,  2.0000],\n",
      "          [ 2.0000,  1.0000,  0.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  2.0000,  2.0000,  1.0000],\n",
      "          [ 2.0000,  3.0000,  4.0000,  3.0000],\n",
      "          [ 1.0000,  2.0000,  3.0000,  2.0000],\n",
      "          [ 2.0000,  2.0000,  1.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  1.0000,  1.0000],\n",
      "          [ 0.0000,  1.0000,  3.0000,  1.0000],\n",
      "          [ 0.0000,  0.0000,  1.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 4.0000,  3.0000,  5.0000,  4.0000],\n",
      "          [ 6.0000,  0.0000,  1.0000,  0.0000],\n",
      "          [13.0000, 15.0000, 14.0000,  1.0000],\n",
      "          [ 8.0000, 11.0000, 11.0000,  3.0000]],\n",
      "\n",
      "         [[ 2.0000,  3.0000,  1.0000,  4.0000],\n",
      "          [ 3.0000,  9.0000, 12.0000, 11.0000],\n",
      "          [ 0.0000,  5.0000, 15.0000, 12.0000],\n",
      "          [ 1.0000,  3.0000,  6.0000,  5.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 6.0000,  2.0000,  0.0000,  0.0000],\n",
      "          [ 9.0000, 11.0000,  4.0000,  0.0000],\n",
      "          [ 5.0000,  5.0000,  2.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 7.0000,  9.0000,  5.0000,  1.0000],\n",
      "          [ 3.0000,  3.0000,  1.0000,  0.0000]],\n",
      "\n",
      "         [[ 2.0000,  6.0000, 10.0000,  9.0000],\n",
      "          [ 7.0000, 13.0000, 12.0000,  9.0000],\n",
      "          [ 4.0000,  8.0000,  6.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 5.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 7.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 4.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 2.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 1.0000,  6.0000,  7.0000,  6.0000],\n",
      "          [ 1.0000,  5.0000,  3.0000,  0.0000],\n",
      "          [ 1.0000,  1.0000,  0.0000,  0.0000],\n",
      "          [ 2.0000,  2.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.0000, 10.0000, 10.0000,  8.0000],\n",
      "          [ 4.0000,  3.0000,  1.0000,  0.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  3.0000,  3.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 2.0000,  2.0000,  8.0000, 11.0000],\n",
      "          [ 2.0000,  0.0000,  5.0000, 11.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  2.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 6.0000,  9.0000, 10.0000,  5.0000],\n",
      "          [ 6.0000,  7.0000,  7.0000,  6.0000],\n",
      "          [ 7.0000,  7.0000,  8.0000,  6.0000],\n",
      "          [ 6.0000,  3.0000,  3.0000,  1.0000]],\n",
      "\n",
      "         [[ 3.0000,  3.0000,  0.0000,  0.0000],\n",
      "          [ 4.0000,  2.0000,  0.0000,  0.0000],\n",
      "          [ 2.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 3.0000,  2.0000,  2.0000,  2.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 2.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 2.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 1.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  3.0000,  3.0000],\n",
      "          [ 1.0000,  8.0000,  8.0000,  5.0000],\n",
      "          [ 2.0000,  2.0000,  2.0000,  2.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 6.0000,  5.0000, 10.0000,  5.0000],\n",
      "          [ 8.0000,  6.0000, 11.0000,  1.0000],\n",
      "          [10.0000,  7.0000,  8.0000,  0.0000],\n",
      "          [ 7.0000,  3.0000,  4.0000,  1.0000]],\n",
      "\n",
      "         [[ 3.0000,  0.0000,  2.0000,  2.0000],\n",
      "          [ 2.0000,  0.0000,  0.0000,  1.0000],\n",
      "          [ 2.0000,  1.0000,  1.0000,  2.0000],\n",
      "          [ 1.0000,  1.0000,  2.0000,  2.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  2.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  1.0000]],\n",
      "\n",
      "         [[ 0.0000,  2.0000,  4.0000,  0.0000],\n",
      "          [ 1.0000,  2.0000,  3.0000,  0.0000],\n",
      "          [ 3.0000,  3.0000,  4.0000,  1.0000],\n",
      "          [ 2.0000,  2.0000,  2.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  2.0000,  1.0000,  1.0000],\n",
      "          [ 0.0000,  2.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  1.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 3.0000,  1.0000,  2.0000,  3.0000],\n",
      "          [ 4.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 2.0000,  1.0000,  1.0000,  1.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 3.0000,  0.0000,  0.0000,  1.0000],\n",
      "          [ 4.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 2.0000,  2.0000,  2.0000,  1.0000],\n",
      "          [ 6.0000,  6.0000,  6.0000,  2.0000],\n",
      "          [ 4.0000,  5.0000,  5.0000,  2.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 3.0000,  2.0000,  2.0000,  0.0000],\n",
      "          [ 3.0000,  2.0000,  2.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x_bit = 4\n",
    "x = save_output.outputs[8][0]  # input of the 8th conv layer\n",
    "x_alpha  = model.features[27].act_alpha\n",
    "x_delta = x_alpha/(2**x_bit-1)\n",
    "\n",
    "act_quant_fn = act_quantization(x_bit) # define the quantization function\n",
    "x_q = act_quant_fn(x, x_alpha)         # create the quantized value for x\n",
    "\n",
    "x_int = x_q/x_delta\n",
    "print(x_int) # you should see clean integer numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "05fed024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 8, 4, 4])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_int.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ranging-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_int = torch.nn.Conv2d(in_channels = 8, out_channels=8, kernel_size = 3, padding=1, bias = False)\n",
    "conv_int.weight = torch.nn.parameter.Parameter(weight_int)\n",
    "relu = nn.ReLU()\n",
    "output_int = conv_int(x_int)\n",
    "output_recovered = output_int*w_delta*x_delta\n",
    "output_recovered = relu(output_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8d5639a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2645e-07, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "difference = abs(save_output.outputs[9][0] - output_recovered )\n",
    "print(difference.mean())  ## It should be small, e.g.,2.3 in my trainned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a82fa797",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pad = torch.zeros(128, 8, 6, 6).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4e7a382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pad[ : ,  :, 1:5, 1:5] = x_int.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5f94b9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_pad[0]\n",
    "X = torch.reshape(X, (X.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "30aba0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 36])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "91769576",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_id = 0 \n",
    "nij = 200 # just a random number\n",
    "#X = a_tile[tile_id,:,nij:nij+64]  # [tile_num, array row num, time_steps]\n",
    "\n",
    "bit_precision = 4\n",
    "file = open('activation.txt', 'w') #write to file\n",
    "file.write('#time0row7[msb-lsb],time0row6[msb-lst],....,time0row0[msb-lst]#\\n')\n",
    "file.write('#time1row7[msb-lsb],time1row6[msb-lst],....,time1row0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "for i in range(X.size(1)):  # time step\n",
    "    for j in range(X.size(0)): # row #\n",
    "        X_bin = '{0:04b}'.format(int(X[7-j,i].item()+0.001))\n",
    "        for k in range(bit_precision):\n",
    "            file.write(X_bin[k])        \n",
    "        #file.write(' ')  # for visibility with blank between words, you can use\n",
    "    file.write('\\n')\n",
    "file.close() #close file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d8585556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 5., 2., 2., 1., 1., 1., 0.], device='cuda:0',\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "967e165b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8, 3, 3])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_int.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "71400dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8, 9])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_int.size() # 8, 8 , 3, 3\n",
    "W = torch.reshape(weight_int, (weight_int.size(0), weight_int.size(1), -1))\n",
    "W.size() # 8, 8, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "907e42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_precision = 4\n",
    "file = open('weight.txt', 'w') #write to file\n",
    "file.write('#col0row7[msb-lsb],col0row6[msb-lst],....,col0row0[msb-lst]#\\n')\n",
    "file.write('#col1row7[msb-lsb],col1row6[msb-lst],....,col1row0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "for kij in range(9):\n",
    "    for i in range(W.size(0)):  \n",
    "        for j in range(W.size(1)):\n",
    "            if (W[i, 7-j, kij].item()<0):\n",
    "                W_bin = '{0:04b}'.format(int(W[i,7-j,kij].item()+2**bit_precision+0.001))\n",
    "            else:\n",
    "                W_bin = '{0:04b}'.format(int(W[i,7-j,kij].item()+0.001))\n",
    "            for k in range(bit_precision):\n",
    "                file.write(W_bin[k])        \n",
    "            #file.write(' ')  # for visibility with blank between words, you can use\n",
    "        file.write('\\n')\n",
    "file.close() #close file  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cc656e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.0000,  7.0000,  7.0000,  7.0000, -7.0000,  7.0000, -7.0000, -7.0000],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6ec9b1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_nijg = range(X.size(1)) ## psum nij group\n",
    "\n",
    "psum = torch.zeros(8, len(p_nijg), 9).cuda() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bb4a9b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 36, 9])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psum.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "af4b6b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "for kij in range(9):  \n",
    "    for nij in p_nijg:       # time domain, sequentially given input\n",
    "        m = nn.Linear(8, 8, bias=False)\n",
    "        m.weight = torch.nn.Parameter(W[:,:,kij])\n",
    "        psum[:, nij, kij] = m(X[:,nij]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f57b5212",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "bit_precision = 16\n",
    "file = open('psum.txt', 'w') #write to file\n",
    "file.write('#time0col7[msb-lsb],time0col6[msb-lst],....,time0col0[msb-lst]#\\n')\n",
    "file.write('#time1col7[msb-lsb],time1col6[msb-lst],....,time1col0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "for kij in range(9):\n",
    "    for i in range(psum.size(1)):\n",
    "        for j in range(psum.size(0)):\n",
    "            if (psum[7-j,i,kij].item()<0):\n",
    "                P_bin = '{0:016b}'.format(int(psum[7-j,i,kij].item()+2**bit_precision+0.001))\n",
    "            else:\n",
    "                P_bin = '{0:016b}'.format(int(psum[7-j,i,kij].item()+0.001))\n",
    "            for k in range(bit_precision):\n",
    "                file.write(P_bin[k])\n",
    "            #file.write(' ')  # for visibility with blank between words, you can use\n",
    "        file.write('\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1d07748d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 14.0000, -28.0000,  42.0000,  14.0000, -28.0000, -14.0000, -28.0000,\n",
       "        -14.0000], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psum[:,8,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efb31657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor(2., device='cuda:0')\n",
      "tensor(6., device='cuda:0')\n",
      "tensor(7., device='cuda:0')\n",
      "tensor(8., device='cuda:0')\n",
      "tensor(12., device='cuda:0')\n",
      "tensor(13., device='cuda:0')\n",
      "tensor(14., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor(2., device='cuda:0')\n",
      "tensor(3., device='cuda:0')\n",
      "tensor(7., device='cuda:0')\n",
      "tensor(8., device='cuda:0')\n",
      "tensor(9., device='cuda:0')\n",
      "tensor(13., device='cuda:0')\n",
      "tensor(14., device='cuda:0')\n",
      "tensor(15., device='cuda:0')\n",
      "tensor(2., device='cuda:0')\n",
      "tensor(3., device='cuda:0')\n",
      "tensor(4., device='cuda:0')\n",
      "tensor(8., device='cuda:0')\n",
      "tensor(9., device='cuda:0')\n",
      "tensor(10., device='cuda:0')\n",
      "tensor(14., device='cuda:0')\n",
      "tensor(15., device='cuda:0')\n",
      "tensor(16., device='cuda:0')\n",
      "tensor(3., device='cuda:0')\n",
      "tensor(4., device='cuda:0')\n",
      "tensor(5., device='cuda:0')\n",
      "tensor(9., device='cuda:0')\n",
      "tensor(10., device='cuda:0')\n",
      "tensor(11., device='cuda:0')\n",
      "tensor(15., device='cuda:0')\n",
      "tensor(16., device='cuda:0')\n",
      "tensor(17., device='cuda:0')\n",
      "tensor(6., device='cuda:0')\n",
      "tensor(7., device='cuda:0')\n",
      "tensor(8., device='cuda:0')\n",
      "tensor(12., device='cuda:0')\n",
      "tensor(13., device='cuda:0')\n",
      "tensor(14., device='cuda:0')\n",
      "tensor(18., device='cuda:0')\n",
      "tensor(19., device='cuda:0')\n",
      "tensor(20., device='cuda:0')\n",
      "tensor(7., device='cuda:0')\n",
      "tensor(8., device='cuda:0')\n",
      "tensor(9., device='cuda:0')\n",
      "tensor(13., device='cuda:0')\n",
      "tensor(14., device='cuda:0')\n",
      "tensor(15., device='cuda:0')\n",
      "tensor(19., device='cuda:0')\n",
      "tensor(20., device='cuda:0')\n",
      "tensor(21., device='cuda:0')\n",
      "tensor(8., device='cuda:0')\n",
      "tensor(9., device='cuda:0')\n",
      "tensor(10., device='cuda:0')\n",
      "tensor(14., device='cuda:0')\n",
      "tensor(15., device='cuda:0')\n",
      "tensor(16., device='cuda:0')\n",
      "tensor(20., device='cuda:0')\n",
      "tensor(21., device='cuda:0')\n",
      "tensor(22., device='cuda:0')\n",
      "tensor(9., device='cuda:0')\n",
      "tensor(10., device='cuda:0')\n",
      "tensor(11., device='cuda:0')\n",
      "tensor(15., device='cuda:0')\n",
      "tensor(16., device='cuda:0')\n",
      "tensor(17., device='cuda:0')\n",
      "tensor(21., device='cuda:0')\n",
      "tensor(22., device='cuda:0')\n",
      "tensor(23., device='cuda:0')\n",
      "tensor(12., device='cuda:0')\n",
      "tensor(13., device='cuda:0')\n",
      "tensor(14., device='cuda:0')\n",
      "tensor(18., device='cuda:0')\n",
      "tensor(19., device='cuda:0')\n",
      "tensor(20., device='cuda:0')\n",
      "tensor(24., device='cuda:0')\n",
      "tensor(25., device='cuda:0')\n",
      "tensor(26., device='cuda:0')\n",
      "tensor(13., device='cuda:0')\n",
      "tensor(14., device='cuda:0')\n",
      "tensor(15., device='cuda:0')\n",
      "tensor(19., device='cuda:0')\n",
      "tensor(20., device='cuda:0')\n",
      "tensor(21., device='cuda:0')\n",
      "tensor(25., device='cuda:0')\n",
      "tensor(26., device='cuda:0')\n",
      "tensor(27., device='cuda:0')\n",
      "tensor(14., device='cuda:0')\n",
      "tensor(15., device='cuda:0')\n",
      "tensor(16., device='cuda:0')\n",
      "tensor(20., device='cuda:0')\n",
      "tensor(21., device='cuda:0')\n",
      "tensor(22., device='cuda:0')\n",
      "tensor(26., device='cuda:0')\n",
      "tensor(27., device='cuda:0')\n",
      "tensor(28., device='cuda:0')\n",
      "tensor(15., device='cuda:0')\n",
      "tensor(16., device='cuda:0')\n",
      "tensor(17., device='cuda:0')\n",
      "tensor(21., device='cuda:0')\n",
      "tensor(22., device='cuda:0')\n",
      "tensor(23., device='cuda:0')\n",
      "tensor(27., device='cuda:0')\n",
      "tensor(28., device='cuda:0')\n",
      "tensor(29., device='cuda:0')\n",
      "tensor(18., device='cuda:0')\n",
      "tensor(19., device='cuda:0')\n",
      "tensor(20., device='cuda:0')\n",
      "tensor(24., device='cuda:0')\n",
      "tensor(25., device='cuda:0')\n",
      "tensor(26., device='cuda:0')\n",
      "tensor(30., device='cuda:0')\n",
      "tensor(31., device='cuda:0')\n",
      "tensor(32., device='cuda:0')\n",
      "tensor(19., device='cuda:0')\n",
      "tensor(20., device='cuda:0')\n",
      "tensor(21., device='cuda:0')\n",
      "tensor(25., device='cuda:0')\n",
      "tensor(26., device='cuda:0')\n",
      "tensor(27., device='cuda:0')\n",
      "tensor(31., device='cuda:0')\n",
      "tensor(32., device='cuda:0')\n",
      "tensor(33., device='cuda:0')\n",
      "tensor(20., device='cuda:0')\n",
      "tensor(21., device='cuda:0')\n",
      "tensor(22., device='cuda:0')\n",
      "tensor(26., device='cuda:0')\n",
      "tensor(27., device='cuda:0')\n",
      "tensor(28., device='cuda:0')\n",
      "tensor(32., device='cuda:0')\n",
      "tensor(33., device='cuda:0')\n",
      "tensor(34., device='cuda:0')\n",
      "tensor(21., device='cuda:0')\n",
      "tensor(22., device='cuda:0')\n",
      "tensor(23., device='cuda:0')\n",
      "tensor(27., device='cuda:0')\n",
      "tensor(28., device='cuda:0')\n",
      "tensor(29., device='cuda:0')\n",
      "tensor(33., device='cuda:0')\n",
      "tensor(34., device='cuda:0')\n",
      "tensor(35., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "address = torch.zeros(16, 9).cuda()\n",
    "\n",
    "for o_nij in range(16):\n",
    "    for kij in range(9):\n",
    "        address[o_nij, kij] = int(o_nij/4)*6 + o_nij%4 + int(kij/3)*6 + kij%3\n",
    "        #print(address[o_nij, kij])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fa68415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 9])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "address.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba117173",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file = open('acc_address.txt', 'w') #write to file\n",
    "file.write('#1st address#\\n')\n",
    "file.write('#2st address#\\n')\n",
    "file.write('#................#\\n')\n",
    "bit_precision = 11\n",
    "\n",
    "for i in range(address.size(0)):\n",
    "    for j in range(address.size(1)):\n",
    "        a_bin = '{0:011b}'.format(int(address[i, j]))\n",
    "        for k in range(bit_precision):\n",
    "            file.write(a_bin[k])\n",
    "        file.write('\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6c7febf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 8, 4, 4])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_int.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9533828b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 4])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = output_int[0]\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "433f0285",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.reshape(out, (out.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "56071554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "493ae660",
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_precision = 16\n",
    "file = open('output.txt', 'w') #write to file\n",
    "file.write('#time0col7[msb-lsb],time0col6[msb-lst],....,time0col0[msb-lst]#\\n')\n",
    "file.write('#time1col7[msb-lsb],time1col6[msb-lst],....,time1col0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "for i in range(out.size(1)):\n",
    "    for j in range(out.size(0)):\n",
    "        if (out[7-j,i].item()<0):\n",
    "            O_bin = '{0:016b}'.format(int(out[7-j,i].item()+2**bit_precision+0.001))\n",
    "        else:\n",
    "            O_bin = '{0:016b}'.format(int(out[7-j,i].item()+0.001))\n",
    "        for k in range(bit_precision):\n",
    "            file.write(O_bin[k])\n",
    "        #file.write(' ')  # for visibility with blank between words, you can use\n",
    "    file.write('\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "597f0ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -56.0000,  -20.0000,   84.0000, -112.0000,   84.0000,  -76.0000,\n",
       "        -126.0000,  140.0000], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-significance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-witch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-barbados",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-serbia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
