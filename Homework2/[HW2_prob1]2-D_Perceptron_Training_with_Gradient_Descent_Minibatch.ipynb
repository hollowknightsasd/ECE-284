{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "055ddbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fcl): Linear(in_features=2, out_features=1, bias=False)\n",
      ")\n",
      "[Parameter containing:\n",
      "tensor([[ 0.5166, -0.0778]], requires_grad=True)]\n",
      "Epoch 0 - loss: 59.035369873046875\n",
      "Epoch 1 - loss: 68.28572082519531\n",
      "Epoch 2 - loss: 2.538409948348999\n",
      "Epoch 3 - loss: 73.06547546386719\n",
      "Epoch 4 - loss: 34.93756866455078\n",
      "Epoch 5 - loss: 6.433072566986084\n",
      "Epoch 6 - loss: 0.48353108763694763\n",
      "Epoch 7 - loss: 107.81143188476562\n",
      "Epoch 8 - loss: 0.6135016083717346\n",
      "Epoch 9 - loss: 221.79010009765625\n",
      "Epoch 10 - loss: 0.012571249157190323\n",
      "Epoch 11 - loss: 20.085506439208984\n",
      "Epoch 12 - loss: 27.435443878173828\n",
      "Epoch 13 - loss: 3.5192272663116455\n",
      "Epoch 14 - loss: 53.73277282714844\n",
      "Epoch 15 - loss: 64.50284576416016\n",
      "Epoch 16 - loss: 0.9138627648353577\n",
      "Epoch 17 - loss: 212.65357971191406\n",
      "Epoch 18 - loss: 7.931520938873291\n",
      "Epoch 19 - loss: 309.456298828125\n",
      "Epoch 20 - loss: 134.5611572265625\n",
      "Epoch 21 - loss: 226.06607055664062\n",
      "Epoch 22 - loss: 167.22811889648438\n",
      "Epoch 23 - loss: 14.072844505310059\n",
      "Epoch 24 - loss: 0.034963395446538925\n",
      "Epoch 25 - loss: 1.5476925373077393\n",
      "Epoch 26 - loss: 5.748367786407471\n",
      "Epoch 27 - loss: 25.971128463745117\n",
      "Epoch 28 - loss: 2.872209310531616\n",
      "Epoch 29 - loss: 36.90410614013672\n",
      "Epoch 30 - loss: 0.48495393991470337\n",
      "Epoch 31 - loss: 6.172962665557861\n",
      "Epoch 32 - loss: 5.389954566955566\n",
      "Epoch 33 - loss: 1.015734076499939\n",
      "Epoch 34 - loss: 0.7688695192337036\n",
      "Epoch 35 - loss: 0.12403915077447891\n",
      "Epoch 36 - loss: 0.23365192115306854\n",
      "Epoch 37 - loss: 0.022918811067938805\n",
      "Epoch 38 - loss: 0.03345291689038277\n",
      "Epoch 39 - loss: 0.0050168028101325035\n",
      "Epoch 40 - loss: 0.007523623760789633\n",
      "Epoch 41 - loss: 0.2664763033390045\n",
      "Epoch 42 - loss: 0.020657332614064217\n",
      "Epoch 43 - loss: 0.053800489753484726\n",
      "Epoch 44 - loss: 0.07792685925960541\n",
      "Epoch 45 - loss: 0.05639025941491127\n",
      "Epoch 46 - loss: 0.03601490706205368\n",
      "Epoch 47 - loss: 0.01086884830147028\n",
      "Epoch 48 - loss: 0.020190607756376266\n",
      "Epoch 49 - loss: 0.005778062157332897\n",
      "Epoch 50 - loss: 0.0009935935959219933\n",
      "Epoch 51 - loss: 0.04767315462231636\n",
      "Epoch 52 - loss: 0.02286643162369728\n",
      "Epoch 53 - loss: 0.049064114689826965\n",
      "Epoch 54 - loss: 0.0006639673956669867\n",
      "Epoch 55 - loss: 0.209202840924263\n",
      "Epoch 56 - loss: 0.025343334302306175\n",
      "Epoch 57 - loss: 0.00014766425010748208\n",
      "Epoch 58 - loss: 0.060396552085876465\n",
      "Epoch 59 - loss: 0.022857490926980972\n",
      "Epoch 60 - loss: 0.037209928035736084\n",
      "Epoch 61 - loss: 0.003997645806521177\n",
      "Epoch 62 - loss: 0.014702177606523037\n",
      "Epoch 63 - loss: 0.0005365614197216928\n",
      "Epoch 64 - loss: 0.0808480978012085\n",
      "Epoch 65 - loss: 0.25062981247901917\n",
      "Epoch 66 - loss: 0.01278450433164835\n",
      "Epoch 67 - loss: 0.009109504520893097\n",
      "Epoch 68 - loss: 0.11371295899152756\n",
      "Epoch 69 - loss: 0.14291808009147644\n",
      "Epoch 70 - loss: 0.37244054675102234\n",
      "Epoch 71 - loss: 0.19386911392211914\n",
      "Epoch 72 - loss: 0.11419923603534698\n",
      "Epoch 73 - loss: 0.12578880786895752\n",
      "Epoch 74 - loss: 0.10540353506803513\n",
      "Epoch 75 - loss: 0.6724135875701904\n",
      "Epoch 76 - loss: 0.04981667920947075\n",
      "Epoch 77 - loss: 0.0006112293340265751\n",
      "Epoch 78 - loss: 0.0018140013562515378\n",
      "Epoch 79 - loss: 0.002469404833391309\n",
      "Epoch 80 - loss: 0.029302841052412987\n",
      "Epoch 81 - loss: 0.03628816828131676\n",
      "Epoch 82 - loss: 0.0033082112204283476\n",
      "Epoch 83 - loss: 0.005731904413551092\n",
      "Epoch 84 - loss: 0.06763582676649094\n",
      "Epoch 85 - loss: 0.04559244215488434\n",
      "Epoch 86 - loss: 0.21846772730350494\n",
      "Epoch 87 - loss: 0.01137795951217413\n",
      "Epoch 88 - loss: 0.014909078367054462\n",
      "Epoch 89 - loss: 0.060150012373924255\n",
      "Epoch 90 - loss: 0.03102860040962696\n",
      "Epoch 91 - loss: 0.001426449278369546\n",
      "Epoch 92 - loss: 0.0005252668634057045\n",
      "Epoch 93 - loss: 0.0020259511657059193\n",
      "Epoch 94 - loss: 0.009495437145233154\n",
      "Epoch 95 - loss: 0.0033717032056301832\n",
      "Epoch 96 - loss: 0.00023830487043596804\n",
      "Epoch 97 - loss: 0.0010090142022818327\n",
      "Epoch 98 - loss: 0.0007234717486426234\n",
      "Epoch 99 - loss: 0.008113938383758068\n",
      "Epoch 100 - loss: 0.00012673465244006366\n",
      "Epoch 101 - loss: 0.00020778609905391932\n",
      "Epoch 102 - loss: 0.0002828306460287422\n",
      "Epoch 103 - loss: 0.0007874148432165384\n",
      "Epoch 104 - loss: 0.006228875368833542\n",
      "Epoch 105 - loss: 0.0011994887609034777\n",
      "Epoch 106 - loss: 0.0009291120222769678\n",
      "Epoch 107 - loss: 0.00012580232578329742\n",
      "Epoch 108 - loss: 0.00012944373884238303\n",
      "Epoch 109 - loss: 0.0011150861391797662\n",
      "Epoch 110 - loss: 0.00020463630789890885\n",
      "Epoch 111 - loss: 0.00019040450570173562\n",
      "Epoch 112 - loss: 0.00034966785460710526\n",
      "Epoch 113 - loss: 0.0001942132948897779\n",
      "Epoch 114 - loss: 7.9612378613092e-05\n",
      "Epoch 115 - loss: 0.002310995478183031\n",
      "Epoch 116 - loss: 0.0009520715102553368\n",
      "Epoch 117 - loss: 0.005618964787572622\n",
      "Epoch 118 - loss: 0.0026204546447843313\n",
      "Epoch 119 - loss: 2.7853417122969404e-05\n",
      "Epoch 120 - loss: 0.0005511955823749304\n",
      "Epoch 121 - loss: 0.0004558592336252332\n",
      "Epoch 122 - loss: 1.1465135685284622e-05\n",
      "Epoch 123 - loss: 8.827728743199259e-05\n",
      "Epoch 124 - loss: 0.0001134258636739105\n",
      "Epoch 125 - loss: 1.879158844531048e-05\n",
      "Epoch 126 - loss: 7.171242032200098e-06\n",
      "Epoch 127 - loss: 1.2579185749928001e-05\n",
      "Epoch 128 - loss: 1.8499213183531538e-05\n",
      "Epoch 129 - loss: 4.318318133300636e-06\n",
      "Epoch 130 - loss: 1.563167643325869e-06\n",
      "Epoch 131 - loss: 8.995177722681547e-07\n",
      "Epoch 132 - loss: 3.521518578963878e-07\n",
      "Epoch 133 - loss: 4.126286512473598e-08\n",
      "Epoch 134 - loss: 1.7851571101346053e-06\n",
      "Epoch 135 - loss: 5.593829541794548e-07\n",
      "Epoch 136 - loss: 1.1582526440179208e-06\n",
      "Epoch 137 - loss: 8.842071110848337e-07\n",
      "Epoch 138 - loss: 7.79726542532444e-06\n",
      "Epoch 139 - loss: 1.0793905858008657e-05\n",
      "Epoch 140 - loss: 1.150715343101183e-05\n",
      "Epoch 141 - loss: 1.4381896562554175e-08\n",
      "Epoch 142 - loss: 1.1552489013411105e-05\n",
      "Epoch 143 - loss: 1.4657691735919798e-06\n",
      "Epoch 144 - loss: 2.1681771613657475e-06\n",
      "Epoch 145 - loss: 6.805066732340492e-07\n",
      "Epoch 146 - loss: 2.769434104266111e-06\n",
      "Epoch 147 - loss: 6.636974489993008e-07\n",
      "Epoch 148 - loss: 3.501048922771588e-06\n",
      "Epoch 149 - loss: 1.992157194763422e-08\n",
      "Epoch 150 - loss: 7.968628779053688e-08\n",
      "Epoch 151 - loss: 3.885806108883116e-06\n",
      "Epoch 152 - loss: 1.9360240912646987e-06\n",
      "Epoch 153 - loss: 6.10098140896298e-08\n",
      "Epoch 154 - loss: 1.6600106391706504e-06\n",
      "Epoch 155 - loss: 3.575487426132895e-07\n",
      "Epoch 156 - loss: 1.848280953709036e-05\n",
      "Epoch 157 - loss: 1.2967735528945923e-05\n",
      "Epoch 158 - loss: 8.550379425287247e-05\n",
      "Epoch 159 - loss: 9.231913281837478e-06\n",
      "Epoch 160 - loss: 0.0003583974903449416\n",
      "Epoch 161 - loss: 1.0594311788736377e-05\n",
      "Epoch 162 - loss: 1.743018947308883e-05\n",
      "Epoch 163 - loss: 8.045702270464972e-05\n",
      "Epoch 164 - loss: 1.4716084479005076e-05\n",
      "Epoch 165 - loss: 3.342196941957809e-05\n",
      "Epoch 166 - loss: 5.071962732472457e-06\n",
      "Epoch 167 - loss: 3.296045179013163e-07\n",
      "Epoch 168 - loss: 1.8223001916339854e-06\n",
      "Epoch 169 - loss: 2.709453838178888e-06\n",
      "Epoch 170 - loss: 6.077448233554605e-06\n",
      "Epoch 171 - loss: 2.378045792283956e-06\n",
      "Epoch 172 - loss: 3.7573732925011427e-07\n",
      "Epoch 173 - loss: 3.6927531255059876e-06\n",
      "Epoch 174 - loss: 9.648829291108996e-07\n",
      "Epoch 175 - loss: 1.8499213183531538e-07\n",
      "Epoch 176 - loss: 4.926696419715881e-07\n",
      "Epoch 177 - loss: 6.148184183984995e-08\n",
      "Epoch 178 - loss: 1.2789769243681803e-07\n",
      "Epoch 179 - loss: 9.139421308645979e-08\n",
      "Epoch 180 - loss: 3.84081886295462e-08\n",
      "Epoch 181 - loss: 4.165121936239302e-08\n",
      "Epoch 182 - loss: 2.954948286060244e-07\n",
      "Epoch 183 - loss: 1.542963445899659e-08\n",
      "Epoch 184 - loss: 1.126900315284729e-07\n",
      "Epoch 185 - loss: 4.604316927725449e-08\n",
      "Epoch 186 - loss: 7.258336154336575e-08\n",
      "Epoch 187 - loss: 6.581194611499086e-08\n",
      "Epoch 188 - loss: 2.030503765126923e-09\n",
      "Epoch 189 - loss: 1.5739351511001587e-07\n",
      "Epoch 190 - loss: 5.676156433764845e-09\n",
      "Epoch 191 - loss: 2.722026692936197e-08\n",
      "Epoch 192 - loss: 6.386926543200389e-10\n",
      "Epoch 193 - loss: 4.0108716348186135e-10\n",
      "Epoch 194 - loss: 3.637978807091713e-12\n",
      "Epoch 195 - loss: 9.094947017729282e-11\n",
      "Epoch 196 - loss: 2.270462573505938e-08\n",
      "Epoch 197 - loss: 7.614517016918398e-09\n",
      "Epoch 198 - loss: 1.499174686614424e-07\n",
      "Epoch 199 - loss: 1.707030605757609e-08\n",
      "Epoch 200 - loss: 6.53235474601388e-08\n",
      "Epoch 201 - loss: 4.645335138775408e-08\n",
      "Epoch 202 - loss: 1.005503236228833e-07\n",
      "Epoch 203 - loss: 3.0174851417541504e-07\n",
      "Epoch 204 - loss: 1.5176942724792752e-07\n",
      "Epoch 205 - loss: 9.488667274126783e-08\n",
      "Epoch 206 - loss: 2.888122253352776e-06\n",
      "Epoch 207 - loss: 6.757945811841637e-07\n",
      "Epoch 208 - loss: 2.447668748573051e-06\n",
      "Epoch 209 - loss: 1.766095238053822e-06\n",
      "Epoch 210 - loss: 2.54256883636117e-06\n",
      "Epoch 211 - loss: 9.058603609446436e-07\n",
      "Epoch 212 - loss: 9.047394996741787e-06\n",
      "Epoch 213 - loss: 1.7095235307351686e-06\n",
      "Epoch 214 - loss: 5.4485644795931876e-05\n",
      "Epoch 215 - loss: 0.0001358578447252512\n",
      "Epoch 216 - loss: 1.4334455045172945e-05\n",
      "Epoch 217 - loss: 4.1506409615976736e-05\n",
      "Epoch 218 - loss: 0.00014815139002166688\n",
      "Epoch 219 - loss: 1.072820850822609e-05\n",
      "Epoch 220 - loss: 5.4527889005839825e-05\n",
      "Epoch 221 - loss: 3.713657861226238e-05\n",
      "Epoch 222 - loss: 1.5792364138178527e-05\n",
      "Epoch 223 - loss: 1.0315716281183995e-06\n",
      "Epoch 224 - loss: 7.781864042044617e-09\n",
      "Epoch 225 - loss: 4.401954356580973e-06\n",
      "Epoch 226 - loss: 6.148184183984995e-08\n",
      "Epoch 227 - loss: 1.0766088962554932e-06\n",
      "Epoch 228 - loss: 1.9077924662269652e-07\n",
      "Epoch 229 - loss: 5.675250577041879e-06\n",
      "Epoch 230 - loss: 1.7289130482822657e-07\n",
      "Epoch 231 - loss: 1.5288605936802924e-07\n",
      "Epoch 232 - loss: 5.996698746457696e-07\n",
      "Epoch 233 - loss: 4.953508323524147e-07\n",
      "Epoch 234 - loss: 9.665745892561972e-06\n",
      "Epoch 235 - loss: 1.8820537661667913e-05\n",
      "Epoch 236 - loss: 5.676156433764845e-07\n",
      "Epoch 237 - loss: 4.895264282822609e-08\n",
      "Epoch 238 - loss: 3.876411938108504e-06\n",
      "Epoch 239 - loss: 2.796479975586408e-06\n",
      "Epoch 240 - loss: 9.611394489184022e-07\n",
      "Epoch 241 - loss: 6.243135430850089e-08\n",
      "Epoch 242 - loss: 1.916132532642223e-07\n",
      "Epoch 243 - loss: 3.0458068067673594e-08\n",
      "Epoch 244 - loss: 4.087632987648249e-08\n",
      "Epoch 245 - loss: 1.1721658665919676e-07\n",
      "Epoch 246 - loss: 7.139497029129416e-07\n",
      "Epoch 247 - loss: 3.5869015846401453e-07\n",
      "Epoch 248 - loss: 9.094947017729282e-09\n",
      "Epoch 249 - loss: 1.692111254669726e-06\n",
      "Epoch 250 - loss: 1.4551915228366852e-11\n",
      "Epoch 251 - loss: 4.110803274670616e-06\n",
      "Epoch 252 - loss: 1.7698994270176627e-06\n",
      "Epoch 253 - loss: 6.029977726029756e-07\n",
      "Epoch 254 - loss: 3.934837877750397e-06\n",
      "Epoch 255 - loss: 1.1633901522145607e-06\n",
      "Epoch 256 - loss: 2.0943900835845852e-08\n",
      "Epoch 257 - loss: 2.416436473140493e-08\n",
      "Epoch 258 - loss: 1.9386789062991738e-08\n",
      "Epoch 259 - loss: 1.707030605757609e-08\n",
      "Epoch 260 - loss: 3.079185262322426e-08\n",
      "Epoch 261 - loss: 4.456524038687348e-09\n",
      "Epoch 262 - loss: 1.63308868650347e-08\n",
      "Epoch 263 - loss: 3.439936335780658e-09\n",
      "Epoch 264 - loss: 7.697963155806065e-09\n",
      "Epoch 265 - loss: 1.0608346201479435e-08\n",
      "Epoch 266 - loss: 9.277755452785641e-09\n",
      "Epoch 267 - loss: 1.1510792319313623e-10\n",
      "Epoch 268 - loss: 2.0463630789890885e-10\n",
      "Epoch 269 - loss: 1.7826096154749393e-10\n",
      "Epoch 270 - loss: 3.637978807091713e-12\n",
      "Epoch 271 - loss: 3.283275873400271e-10\n",
      "Epoch 272 - loss: 1.5370460459962487e-10\n",
      "Epoch 273 - loss: 2.751221472863108e-11\n",
      "Epoch 274 - loss: 8.185452315956354e-12\n",
      "Epoch 275 - loss: 1.4551915228366852e-11\n",
      "Epoch 276 - loss: 2.2737367544323206e-11\n",
      "Epoch 277 - loss: 5.820766091346741e-11\n",
      "Epoch 278 - loss: 2.9467628337442875e-10\n",
      "Epoch 279 - loss: 3.842615114990622e-11\n",
      "Epoch 280 - loss: 8.185452315956354e-10\n",
      "Epoch 281 - loss: 4.4019543565809727e-10\n",
      "Epoch 282 - loss: 2.0954757928848267e-09\n",
      "Epoch 283 - loss: 5.820766091346741e-11\n",
      "Epoch 284 - loss: 1.924490788951516e-09\n",
      "Epoch 285 - loss: 3.2741809263825417e-11\n",
      "Epoch 286 - loss: 1.178705133497715e-09\n",
      "Epoch 287 - loss: 9.094947017729282e-13\n",
      "Epoch 288 - loss: 9.094947017729282e-11\n",
      "Epoch 289 - loss: 6.963318810448982e-11\n",
      "Epoch 290 - loss: 6.190248313941993e-11\n",
      "Epoch 291 - loss: 7.366907084360719e-11\n",
      "Epoch 292 - loss: 5.238689482212067e-10\n",
      "Epoch 293 - loss: 6.963318810448982e-11\n",
      "Epoch 294 - loss: 2.0463630789890885e-10\n",
      "Epoch 295 - loss: 9.313225746154785e-10\n",
      "Epoch 296 - loss: 3.637978807091713e-10\n",
      "Epoch 297 - loss: 2.0520474208751693e-11\n",
      "Epoch 298 - loss: 2.2737367544323206e-11\n",
      "Epoch 299 - loss: 1.4551915228366852e-11\n",
      "when x = tensor([[1.0000, 2.1000]])， y = tensor([[3.0000]], grad_fn=<MmBackward>)\n",
      "when x = tensor([[2.0000, 3.5000]])， y = tensor([[6.0000]], grad_fn=<MmBackward>)\n",
      "when x = tensor([[3., 3.]])， y = tensor([[9.0000]], grad_fn=<MmBackward>)\n",
      "when x = tensor([[4.0000, 2.1000]])， y = tensor([[12.0000]], grad_fn=<MmBackward>)\n",
      "when x = tensor([[5.0000, 7.2000]])， y = tensor([[15.0000]], grad_fn=<MmBackward>)\n",
      "when x = tensor([[ 6.0000, 10.1000]])， y = tensor([[18.0000]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torch.optim as optim\n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "data = [(1.0,2.1,3.0), (2.0, 3.5, 6.0), (3.0, 3.0, 9.0), (4.0, 2.1, 12.0), (5.0, 7.2, 15.0), (6.0, 10.1, 18.0)]\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.fcl = nn.Linear(2,1,bias=False)\n",
    "    def forward(self,x):    \n",
    "        x = self.fcl(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "print(net)\n",
    "print(list(net.parameters()))\n",
    "\n",
    "#input = torch.randn(1)\n",
    "#out = net(input)\n",
    "\n",
    "#def criterion(out, label):\n",
    "#    return (label - out)**2\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.5)\n",
    "#optimizer = optim.Adam(net.parameters(), lr=0.005)\n",
    "for epoch in range(300) : #0-29\n",
    "    my_data = random.sample(data,3)\n",
    "    for i, current_data in enumerate(my_data):\n",
    "        X, Y = current_data[:2], current_data[2]\n",
    "        X, Y = torch.FloatTensor([X]), torch.FloatTensor([Y])\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(X)\n",
    "        loss = criterion(outputs, Y)\n",
    "        loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"Epoch {} - loss: {}\".format(epoch, loss))\n",
    "        \n",
    "###Test the trained network\n",
    "for i, current_data in enumerate(data):\n",
    "    X, Y = current_data[:2], current_data[2]\n",
    "    X, Y = torch.FloatTensor([X]), torch.FloatTensor([Y])\n",
    "    out = net(torch.FloatTensor(X))\n",
    "    print(\"when x = {}， y = {}\".format(X, out))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6108a629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a9e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49196b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
